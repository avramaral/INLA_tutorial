---
title: "INLA Tutorial"
author: "[André V. Ribeiro Amaral](https://www.avramaral.com/)"
format: 
  html:
    toc: true
    html-math-method: katex
    css: styles.css
---

{{< fa brands github >}} <a href="https://github.com/avramaral/INLA_tutorial" style="text-decoration:none;">GitHub repository</a>

```{r, echo = F}
knitr::opts_knit$set(global.par = T)

options(dplyr.summarise.inform = FALSE)
```

In this tutorial, we will use the following `R` packages.

```{r, result = "hide", message = F, warning = F}
library("INLA")
library("tidyverse")
library("spatstat")
library("raster")
library("sf")
library("rgeos")

library("MASS")
library("spdep")
library("patchwork")
```

To install `INLA`, you can follow the instruction [here](https://www.r-inla.org/download-install). Alternatively,

```{r, eval = F}
install.packages("INLA", repos = c(getOption("repos"), INLA = "https://inla.r-inla-download.org/R/stable"), dep = TRUE) 
```

Along the tutorial, we will use the following common theme and colors for the plots with `ggplot2`

```{r, eval = F}
pal <- c("#00008FFF", "#0000F2FF", "#0063FFFF", "#00D4FFFF", "#46FFB8FF", "#B8FF46FF", "#FFD400FF", "#FF6300FF", "#F00000FF", "#800000FF")

custom_theme <-  theme_bw() + theme(legend.position = "right", 
                              text = element_text(size = 14),
                              plot.title = element_text(size = 16),
                              legend.title = element_text(size = 12))
```

```{r, echo = F}
pal <- c("#00008FFF", "#0000F2FF", "#0063FFFF", "#00D4FFFF", "#46FFB8FF", "#B8FF46FF", "#FFD400FF", "#FF6300FF", "#F00000FF", "#800000FF")

custom_theme <-  theme_bw() + theme(legend.position = "right", 
                              text = element_text(size = 14, family = "LM Roman 10"),
                              plot.title = element_text(size = 16),
                              legend.title = element_text(size = 12))
```

# INLA

In a nutshell, the integrated nested Laplace approximation (INLA) ([Rue et al., 2009](https://doi.org/10.1111/j.1467-9868.2008.00700.x)) method is used to approximate Bayesian inference in latent Gaussian models. In particular, it can be used to fit models of the form \begin{align*}
    y_i|S(x_i), &\theta \sim \pi(y_i|S(x_i), \theta), \text{ for } i \in \{1, \cdots, n\} \\
    S(x)|\theta &\sim \text{Normal}(\mu(\theta), Q(\theta)^{-1}) \\
    \theta &\sim \pi(\theta),
\end{align*} where $y = (y_1, \ldots, y_n)$ is the vector or observed values, $x = (x_1, \ldots, x_n)$ is a Gaussian random field, and $\theta = (\theta_1, \ldots, \theta_k)$, for some $k \in \mathbb{N}$, is a vector of hyperparameters. $\mu(\theta)$ and $Q(\theta)$ represent the mean vector and the precision matrix, respectively.

The observed values $y_i$, $\forall i$, are assumed to have mean $\mu_i=g^{-1}(\eta_i)$, such that the linear prediction $\eta_i$ can be expressed as follows \begin{align*}
  \eta_i = \alpha + \sum^{n_{\beta}}_{k = 1} \beta_k z_{ki} + \sum_{j = 1}^{n_f}f^{(j)}(u_{ij}) + \varepsilon_i, \forall i,
\end{align*} where $\alpha$ is the intercept, $\{\beta_k\}_k$, are the coefficients of covariates $\{z_{ki}\}_{ki}$, and $\{f^{(j)}\}_j$ define some random effects in terms of covariates $\{u_{ij}\}_{ij}$. Lastly, $\varepsilon_i$ is an error term.

In this formulation, $\{f^{(j)}\}_j$ will be commonly defined as spatio(-temporal) structures---at least for the purposes of this tutorial.

# Multiple linear regression

Let us start with a multiple linear regression problem, so that we can get to know the `INLA` notation and how to extract the desired quantities. This example was extracted from [Bayesian Inference with INLA](https://becarioprecario.bitbucket.io/inla-gitbook/ch-INLA.html#multiple-linear-regression) ([Gómez-Rubio, 2021](https://becarioprecario.bitbucket.io/inla-gitbook/index.html)).

We will analyze the `cement` data set from the `MASS` package. `x1`, `x2`, `x3`, and `x4` represent the percentages of the four main chemical ingredients in the setting of 13 cements, and `y` measures the heat evolved.

```{r}
summary(cement)

```

Let us add another data point for prediction.

```{r}
cement <- rbind(cement, data.frame(x1 = 24, x2 = 24, x3 = 24, x4 = 24, y  = NA))
```

And let us fit the following model \begin{align*}
y_i = \alpha + \sum_{j = 1}^{4} \beta_j x_{ji} + \varepsilon_i, \forall i,
\end{align*} where $\varepsilon_i\overset{\text{i.i.d.}}{\sim}\text{Normal}(0, 1/\tau)$.

```{r}
formula_1 <- y ~ 0 + 1 + x1 + x2 + x3 + x4
```

```{r, eval = F}
model_1_1 <- inla(formula = formula_1, 
                  family  = "gaussian",
                  data    = cement,
                  control.predictor = list(compute = TRUE)) # Should the marginals for the linear predictor be computed?
```

```{r, echo = F}
model_1_1 <- readRDS("models/model_1_1.RDS")
```

```{r}
summary(model_1_1)

# model_1_1$summary.fixed              # Estimated fixed effects
# model_1_1$summary.hyperpar           # Estimated hyperparameters
# model_1_1$summary.linear.predictor   # Estimated values for the linear predictor
model_1_1$summary.fitted.values[, 1:5] # Fitted values (+ prediction)
```

`INLA` has numerous functions to process the posterior marginals. For instance, `inla.smarginal()` is used to obtain a spline smoothing.

```{r}
alpha <- model_1_1$marginals.fixed[[1]]

ggplot(data.frame(inla.smarginal(alpha)), aes(x, y)) +
  geom_line() +
  labs(x = "", y = "", title = "Posterior of alpha") + 
  custom_theme
```

`inla.qmarginal()` and `inla.pmarginal()` compute the quantile and distribution function, respectively.

```{r}
qq <- inla.qmarginal(0.05, alpha)
qq

inla.pmarginal(qq, alpha)
```

`inla.dmarginal()` computes the density at particular values.

```{r}
inla.dmarginal(0, alpha)
```

`inla.tmarginal()` transforms the marginal. This is useful, e.g., to obtain the posterior distribution of the $1 / \tau$.

```{r}
sigma_2 <- inla.tmarginal(fun = function(x) { 1 / x }, marginal = model_1_1$marginals.hyperpar$`Precision for the Gaussian observations`)

ggplot(data.frame(inla.smarginal(sigma_2))) +
  geom_line(aes(x, y)) +
  labs(x = "", y = "", title = "Posterior of the variance") + 
  custom_theme
```

Complementary, we can also plot the posterior distribution of the fitted values.

```{r}
post_fitted <- model_1_1$marginals.fitted.values

post_margin <- data.frame(do.call(rbind, post_fitted))
post_margin$cement <- rep(names(post_fitted), times = sapply(post_fitted, nrow))

ggplot(post_margin) + 
  geom_line(aes(x, y)) +
  facet_wrap(~ cement, ncol = 5) +
  labs(x = "", y = "Density") +
  custom_theme
```

# Generalized linear models (for disease mapping)

Next, let us fit a generalized linear model (GLM) for a problem in disease mapping. Initially, we will ignore the underlying spatial structure, but we will consider it in the following part. This example was extracted from [Geospatial Health Data: Modeling and Visualization with R-INLA and Shiny](https://www.paulamoraga.com/book-geospatial/sec-arealdataexamplest.html) ([Moraga, 2019](https://www.paulamoraga.com/book-geospatial/index.html)).

We will model the number of cases of lung cancer in Ohio, USA, from 1968 to 1988. The data can be downloaded from [here](https://github.com/avramaral/INLA_tutorial/raw/main/data/example_2/example_2_data.zip).

After downloading the data, we can do the following

```{r}
# Load data
d_ohio <- read_csv(file = "data/example_2/data_ohio.csv", show_col_types = FALSE)
m_ohio <- read_sf(dsn = "data/example_2/ohio_shapefile/", layer = "fe_2007_39_county")

head(d_ohio)
plot(m_ohio$geometry)
```

For this problem, we would like to model the relative risk of cancer in a certain region. To do so, we can compute first the "standardized incidence ratio" (SIR).

Let $Y_i$ denote the number of cases in the location $i$, then \begin{align*}
  \text{SIR}_i = \frac{Y_i}{E_i}, \forall i,
\end{align*} where \begin{align*}
  E_i = \sum_{j = 1}^{m} r_j^{(s)} \cdot n_j^{(i)},
\end{align*} such that $r_j^{(s)}$ represents the rate in stratum $j$ in the standard population, and $n_j^{(i)}$ is the population in stratum $j$ of location $i$.

We can compute the SIR as follows (`expected()` was extracted from the `SpatialEpi` package)

```{r, message = F}
# Extracted from `SpatialEpi`
expected <- function (population, cases, n.strata, ...) {
  
  n <- length(population) / n.strata
  E <- rep(0, n)
  qNum <- rep(0, n.strata)
  qDenom <- rep(0, n.strata)
  q <- rep(0, n.strata)
  
  # Compute strata-specific rates
  for (i in 1:n.strata) {
    indices <- rep(i, n) + seq(0, (n - 1)) * n.strata
    qNum[i] <- qNum[i] + sum(cases[indices])
    qDenom[i] <- qDenom[i] + sum(population[indices])
  }
  q <- qNum / qDenom
  
  # Compute expected counts
  for (i in 1:n) {
    indices <- (1:n.strata) + (i - 1) * n.strata
    E[i] <- sum(population[indices] * q)
  }
  
  E
}
```

```{r}
################
# Process data #
################

# Observed number of cases
d <- d_ohio %>% group_by(NAME, year) %>% summarise(Y = sum(y)) %>% ungroup() %>% rename(county = NAME) %>% arrange(county, year)

# Expected cases 
d_ohio <- d_ohio %>% arrange(county, year, gender, race)
n.strata <- 4 # 2 genders x 2 races 
E <- expected(population = d_ohio$n, cases = d_ohio$y, n.strata = n.strata)

# Compute SIR
n_years    <- length(unique(d_ohio$year))
n_counties <- length(unique(d_ohio$NAME))

counties_E <- rep(unique(d_ohio$NAME), each = n_years)
years_E    <- rep(unique(d_ohio$year), times = n_counties)

d_E <- data.frame(county = counties_E, year = years_E, E = E)

d <- d %>% left_join(y = d_E, by = c("county", "year"))
d <- d %>% mutate(SIR = Y / E)

# Link map  information
d_wide <- d %>% pivot_wider(names_from = year, values_from = c(Y, E, SIR), names_glue = "{.value}.{year}") # `wide` format
m_ohio <- m_ohio %>% rename(county = NAME) %>% left_join(y = d_wide, by = c("county")) %>% dplyr::select(c(colnames(d_wide), "geometry"))

s_cols <- setdiff(colnames(m_ohio), c("county", "geometry")) # to `long` format
m_ohio <- m_ohio %>% 
          pivot_longer(cols = all_of(s_cols), names_to = c(".value", "year"), names_sep = "\\.") %>% 
          dplyr::select(county, year, Y, E, SIR, geometry) %>%
          mutate(year = as.numeric(year)) %>% 
          arrange(county, year)

head(m_ohio)
```

Now, we can plot the computed SIR for all locations for all years.

```{r}
ggplot(m_ohio) + 
  geom_sf(aes(fill = SIR)) +
  facet_wrap(~ year, ncol = 7) +
  scale_fill_gradient2(name = "SIR", midpoint = 1, low = "blue", mid = "white", high = "red") + 
  labs(x = "", y = "") +
  custom_theme + 
  theme(axis.text.x = element_blank(),
        axis.text.y = element_blank(),
        axis.ticks  = element_blank())
```

However, if there exist locations with small population or expected counts, SIR may be unreliable. To overcome this issue, we will model the relative risk as follows

\begin{align*}
Y_i &\sim \text{Poisson}(E_i \theta_i), \forall i,\\
\log(\theta_i) &= \alpha + \sum_{j = 1}^{n_f}f^{(j)}(u_{ij})
\end{align*} where the relative risk $\theta_i$ quantifies whether the location $i$ has higher ($>1$) or lower ($<1$) risk than the average risk in the standard population. We will experiment with different structures (if any) for the random effects.

Aiming to implement the random effects, let us include an index for area and time in our data set.

```{r}
# Create indices for random effects
m_ohio <- m_ohio %>% mutate(id_area = as.numeric(as.factor(county)),
                            id_time = (1 + year - min(year)))
```

However, we will with data corresponding to $1988$ only (we will consider a spatio-temporal structure later).

```{r}
data <- m_ohio %>% filter(year == 1988)
```

## No random effects

```{r}
formula_1 <- Y ~ 0 + 1
```

```{r, eval = F}
model_2_1 <- inla(formula = formula_1,
                  family  = "poisson", 
                  data = data, 
                  E = E, # Known component in the mean for the Poisson likelihoods
                  control.predictor = list(compute = TRUE),
                  control.compute   = list(cpo  = TRUE, 
                                           dic  = TRUE, 
                                           waic = TRUE)) # For model comparison
```

Notice that we are computing `cpo`, `dic`, and `waic`, so that we can compare models.

```{r, echo = F}
model_2_1 <- readRDS("models/model_2_1.RDS")
```

```{r}
summary(model_2_1)
```

For later use, let us create the function `table_model_comparison` to print a table for model comparison.

```{r}
table_model_comparison <- function (models, ...) {
  
  n_models <- length(models)
  df <- as.data.frame(matrix(data = 0, nrow = n_models, ncol = 3))
  rownames(df) <- names(models)
  colnames(df) <- c("CPO", "DIC", "WAIC")
  
  for (i in 1:n_models) {
    tmp_CPO  <- sum(log(models[[i]]$cpo$cpo)) * -1
    tmp_DIC  <- models[[i]]$dic$dic
    tmp_WAIC <- models[[i]]$waic$waic
    
    df[i, ] <- c(tmp_CPO, tmp_DIC, tmp_WAIC)
  }
  
  df
}
```

## IID model

```{r}
Y ~ 0 + 1 + f(id_area, model = "iid")
```

```{r, eval = F}
model_2_2 <- inla(formula = formula_2,
                  family  = "poisson", 
                  data = data, 
                  E = E, 
                  control.predictor = list(compute = TRUE),
                  control.compute   = list(cpo  = TRUE, 
                                           dic  = TRUE, 
                                           waic = TRUE)) 
```

```{r, echo = F}
model_2_2 <- readRDS("models/model_2_2.RDS")
```

```{r}
summary(model_2_2)
```

```{r}
# Model comparison
table_model_comparison(models = list(model_2_1 = model_2_1, model_2_2 = model_2_2))
```

## Intrinsic Conditional Auto-Regressive (ICAR) model

Alternatively, we can also account for the spatial dependence that may exist underlying our observed counts.

To do so, the first thing we have to do is determining the neighboring structure for the analyzed region. This can be done by using the `poly2nb()` function from the `spdep` package.

```{r}
nb <- poly2nb(data$geometry)
head(nb)

# Save matrix for later use within INLA
nb2INLA("data/example_2/map.adj", nb)
g <- inla.read.graph(filename = "data/example_2/map.adj")
```

Then we can fit a `iid + besag` model.

**Remark:** in `INLA`, we can always use `inla.doc()` to view the documentation (e.g., parameterization) of models.

```{r, eval = F}
# Example: `besag`
inla.doc("besag")
```

In this case, we are fitting a model such that

\begin{align*}
\log(\theta_i) = \alpha + u_i + v_i,
\end{align*} such that $u_i$ is a random effect specific to area $i$ to model spatial dependence between the relative risks, and $v_i$ is an unstructured exchangeable component that models uncorrelated noise.

```{r}
formula_3 <- Y ~ 0 + 1 + f(id_area, model = "iid") + f(id_area_cp, model = "besag", graph = g, scale.model = TRUE)
```

`scale.model = TRUE` makes the model to be scaled to have an average variance of 1 ([Sorbye and Rue, 2014](https://www.sciencedirect.com/science/article/pii/S2211675313000407)).

```{r, eval = F}
model_2_3 <- inla(formula = formula_3,
                  family  = "poisson", 
                  data = data, 
                  E = E, 
                  control.predictor = list(compute = TRUE),
                  control.compute   = list(cpo  = TRUE, 
                                           dic  = TRUE, 
                                           waic = TRUE)) 
```

```{r, echo = F}
model_2_3 <- readRDS("models/model_2_3.RDS")
```

```{r}
summary(model_2_3)
```

```{r}
# Model comparison
table_model_comparison(models = list(model_2_1 = model_2_1, model_2_2 = model_2_2, model_2_3 = model_2_3))
```

## BYM2 model

As an alternative to `iid + besag`, we can fit a "Besag, York, and Mollié" (BYM2) model. In this case, we reparameterize the structured $u = (u_1, \cdots, u_n)$ and unstructured $v = (v_1, \cdots, v_n)$ random effects as follows \begin{align*}
b = u + v = \frac{1}{\sqrt{\tau_b}}(\sqrt{1 - \phi}v^{\star} + \sqrt{\phi}u^{\star}),
\end{align*} where $\tau_b > 0$ is the precision, $0 \leq \phi \leq 1$ is a mixing parameter, $v^{\star} \sim \text{Normal}(0, I_n)$ and $u^{\star}$ is a scaled ICAR model.

```{r}
formula_4 <- Y ~ 0 + 1 + f(id_area, model = "bym2", graph = g)
```

```{r, eval = F}
model_2_4 <- inla(formula = formula_4,
                  family  = "poisson", 
                  data = data, 
                  E = E, 
                  control.predictor = list(compute = TRUE),
                  control.compute   = list(cpo  = TRUE, 
                                           dic  = TRUE, 
                                           waic = TRUE)) 
```

```{r, echo = F}
model_2_4 <- readRDS("models/model_2_4.RDS")
```

```{r}
summary(model_2_4)
```

```{r}
# Model comparison
table_model_comparison(models = list(model_2_1 = model_2_1, model_2_2 = model_2_2, model_2_3 = model_2_3, model_2_4 = model_2_4))
```

### Penalized Complexity (PC) priors

As before, we can also set different priors before fitting our model. In particular, we can set Penalized Complexity (PC) priors for our `bym2` model.

As in [Gómez-Rubio (2021)](https://becarioprecario.bitbucket.io/inla-gitbook/ch-priors.html#sec:pcpriors),

> PC priors are designed following some principles for inference. First of all, the prior favors the base model unless evidence is provided against it, following the principle of parsimony. Distance from the base model is measured using the Kullback-Leibler distance, and penalization from the base model is done at a constant rate on the distance. Finally, the PC prior is defined using probability statements on the model parameters in the appropriate scale.

To define the prior for the marginal precision $\tau_b$ we use the probability statement $\mathbb{P}((1/\sqrt{\tau_b}) > U) = \alpha$. A prior for $\phi$ is defined using $\mathbb{P}(\phi < U) = \alpha$.

```{r}
pc_prior <- list(
  prec = list(
    prior = "pc.prec",
    param = c(1, 0.01)), # P(1 / sqrt(prec) > 1) = 0.01
  phi = list(
    prior = "pc",
    param = c(0.5, 0.75)) # P(phi < 0.5) = 0.75
)
```

From above, notice that $\mathbb{P}(\phi < 0.5) = 0.75$ implies that we believe that the unstructured effect accounts for more of the variability than the spatially structured effect.

```{r}
formula_5 <- Y ~ 0 + 1 + f(id_area, model = "bym2", graph = g, hyper = pc_prior)
```

```{r, eval = F}
model_2_5 <- inla(formula = formula_5,
                  family  = "poisson", 
                  data = data, 
                  E = E, 
                  control.predictor = list(compute = TRUE),
                  control.compute   = list(cpo  = TRUE, 
                                           dic  = TRUE, 
                                           waic = TRUE)) 
```

```{r, echo = F}
model_2_5 <- readRDS("models/model_2_5.RDS")
```

```{r}
summary(model_2_5)
```

```{r}
# Model comparison
table_model_comparison(models = list(model_2_1 = model_2_1, model_2_2 = model_2_2, model_2_3 = model_2_3, model_2_4 = model_2_4, model_2_5 = model_2_5))
```

## Fitted values

Although `model_2_4` and `model_2_5`, for the purpose of this tutorial, I plot the fitted values based on `model_2_3` (`iid + besag`), so that we can plot the estimated unstructured and structured effects separately.

We can retrieve the fitted values by analyzing `model_2_3$summary.fitted.values`.

```{r}
fitted_values <- model_2_3$summary.fitted.values[, c("mean", "0.025quant", "0.975quant")] %>% 
                 as_tibble() %>% 
                 mutate(id_area = 1:nrow(data)) %>% 
                 rename(Mean = mean, `0.025` = `0.025quant`, `0.975` = `0.975quant`) %>% 
                 left_join(y = data[, c("county", "geometry", "id_area")], by = "id_area") %>% 
                 dplyr::select(county, Mean, `0.025`, `0.975`, geometry) %>% 
                 pivot_longer(cols = c(Mean, `0.025`, `0.975`)) %>% 
                 mutate(name = factor(name, levels = c("0.025", "Mean", "0.975"))) %>% 
                 st_as_sf()

ggplot(fitted_values) + 
  geom_sf(aes(fill = value)) +
  facet_wrap(~ name, dir = "h", ncol = 3) +
  scale_fill_gradient2(name = "Relative risk", midpoint = 1, low = "blue", mid = "white", high = "red") + 
  labs(x = "", y = "", title = "Estimated \"Relative Risk\" in 1988") +
  custom_theme + 
  theme(axis.text.x = element_blank(),
        axis.text.y = element_blank(),
        axis.ticks  = element_blank())
```

And we can retrieve the summary of the posterior distribution of the random effects by analyzing `model_2_3$summary.random`.

```{r}
random_effects <- data[, c("county", "geometry")] %>% 
                  mutate("IID" = model_2_3$summary.random$id_area$mean,
                         "BESAG" = model_2_3$summary.random$id_area_cp$mean) %>% 
                  pivot_longer(cols = c(IID, BESAG)) %>% 
                  mutate(name = factor(name, levels = c("IID", "BESAG"))) %>% 
                  st_as_sf()

ggplot(random_effects) + 
  geom_sf(aes(fill = value)) +
  facet_wrap(~ name, dir = "h", ncol = 2) +
  scale_fill_gradient2(name = "RE", midpoint = 0, low = "blue", mid = "white", high = "red") + 
  labs(x = "", y = "", title = "Posterior mean of the random effects") +
  custom_theme + 
  theme(axis.text.x = element_blank(),
        axis.text.y = element_blank(),
        axis.ticks  = element_blank())
```

# Spatio-temporal models

We will continue analyzing the data set we introduced in the previous section (i.e., number of cases of lung cancer in Ohio, USA, from 1968 to 1988), but now we will also account for the possible temporal dependence that may exist over the years.

```{r}
data <- m_ohio
head(data)
```

Similar to before, we will model the relative risk as follows \begin{align*}
Y_{it} &\sim \text{Poisson}(E_{it} \theta_{it}), \forall i, t, \\
\log(\theta_{it}) &= \alpha + \sum_{j = 1}^{n_f}f^{(j)}(u_{it,j}).
\end{align*} However, notice that now all quantities depend on the year (or time, $t$). The random effects may depend on both space and time with possible interaction.

## Replicates

An alternative to model space-time observations is treating them as replicates. In `INLA`, using `replicate` implies that replicated effects share the hyperparameters (only). This means that the values of the random effects in the different replicates can be different ([Gómez-Rubio, 2021](https://becarioprecario.bitbucket.io/inla-gitbook/ch-INLAfeatures.html#subsec:replicate)).

```{r}
formula_1 <- Y ~ 0 + 1 + f(id_area, model = "besag", graph = g, replicate = id_time)
```

```{r, eval = F}
model_3_1 <- inla(formula = formula_1,
                  family  = "poisson", 
                  data = data, 
                  E = E,
                  control.predictor = list(compute = TRUE))
```

```{r, echo = F}
model_3_1 <- readRDS("models/model_3_1.RDS")
```

```{r}
summary(model_3_1)
```

Aiming to plot the fitted values for the different spatio-temporal approaches, we will use the function `plot_rr_years()`.

```{r}
plot_rr_years <- function (d, col_name = "fitted.values", temporal_name = "year", tt = "", ...) {
  ggplot(d) + 
    geom_sf(aes(fill = .data[[col_name]])) +
    facet_wrap(~ .data[[temporal_name]], dir = "h", ncol = 7) +
    scale_fill_gradient2(name = "Relative risk", midpoint = 1, low = "blue", mid = "white", high = "red") + 
    labs(x = "", y = "", title = tt) +
    custom_theme + 
    theme(axis.text.x = element_blank(),
          axis.text.y = element_blank(),
          axis.ticks  = element_blank())
}
```

```{r}
data_3_1 <- bind_cols(data[, c("county", "year", "geometry")], fitted.values = model_3_1$summary.fitted.values$mean)

plot_rr_years(d = data_3_1, tt = "Replicates")
```

## Additive model

Alternatively, we can fit a model with an additive structure for random effects in space and time. For instance,

\begin{align*}
\log(\theta_{it}) = \alpha + u_i + \varrho_t,
\end{align*} where $u_i \sim \text{ICAR}$ and, e.g., $\varrho_t \sim \text{AR}(1)$.

```{r}
formula_2 <- Y ~ 0 + 1 + f(id_time, model = "rw1") + f(id_area, model = "besag", graph = g)
```

```{r, eval = F}
model_3_2 <- inla(formula = formula_2,
                  family  = "poisson", 
                  data = data, 
                  E = E,
                  control.predictor = list(compute = TRUE))
```

```{r, echo = F}
model_3_2 <- readRDS("models/model_3_2.RDS")
```

```{r}
summary(model_3_2)
```

```{r}
data_3_2 <- bind_cols(data[, c("county", "year", "geometry")], fitted.values = model_3_2$summary.fitted.values$mean)

plot_rr_years(d = data_3_2, tt = "Additive model")
```

## Kronecker product model

A more elaborated solution is to consider a spatio-temporal random effect. In particular, we will fit a grouped (or Kronecker) separable model. A more rigorous motivation for this approach is available [here](https://github.com/hrue/r-inla/blob/devel/internal-doc/group/group-models.pdf).

In a nutshell, in the grouped random effected,

1.  There is a **within** group correlation structure, and
2.  There is a **between** group correlation structure.

In particular, if $y_{g, i}$ is the $i$-the element in group $g$, then $\text{Cov}(y_{g_1, i_1}, y_{g_2, i_2}) = (\text{cov. between groups } g_1 \text{ and } g_2) \times (\text{cov. between elements } g_1 \text{ and } g_2)$ ([Simpson, 2016](http://faculty.washington.edu/jonno/SISMIDmaterial/8-Groupedmodels.pdf)).

In `INLA`, the within-group effect is the one defined in the main `f()` function, while the between effect is the one defined using the `control.group` argument.

```{r}
formula_3 <- Y ~ 0 + 1 + f(id_area, model = "besag", graph = g, 
                           group = id_time, control.group = list(model = "rw1"))
```

```{r, eval = F}
model_3_3 <- inla(formula = formula_3,
                  family  = "poisson", 
                  data = data, 
                  E = E,
                  control.predictor = list(compute = TRUE))
```

```{r, echo = F}
model_3_3 <- readRDS("models/model_3_3.RDS")
```

```{r}
summary(model_3_3)
```

```{r}
data_3_3 <- bind_cols(data[, c("county", "year", "geometry")], fitted.values = model_3_3$summary.fitted.values$mean)

plot_rr_years(d = data_3_3, tt = "Kronecker product model (grouped by \"time\")")
```

## Bernardinelli model

Lastly, we can also implement the Bernardinelli model ([Bernardinelli et al., 1995](https://onlinelibrary.wiley.com/doi/10.1002/sim.4780142112)) using `INLA`. In this case, the linear predictor will be defined as follows

\begin{align*}
\log(\theta_{it}) = \alpha + u_i + v_i + (\beta + \delta_i) \times t_j,
\end{align*} where $u_i + v_i$ is a `iid + besag` model, $\beta$ is the global linear trend, and $\delta_i$ denotes a space-time interaction; in particular, this model allows each of the areas to have its own time trend with spatial intercept given by $(\alpha + u_i + v_i)$ and slope given by $(\beta + \delta_i)$ ([Moraga, 2019](https://www.paulamoraga.com/book-geospatial/sec-arealdatatheory.html#spatio-temporal-small-area-disease-risk-estimation)).

```{r}
formula_4 <- Y ~ 0 + 1 + f(id_area, model = "bym", graph = g) + f(id_area_cp, id_time, model = "iid") + id_time
```

```{r, eval = F}
model_3_4 <- inla(formula = formula_4,
                  family  = "poisson", 
                  data = data, 
                  E = E,
                  control.predictor = list(compute = TRUE))
```

```{r, echo = F}
model_3_4 <- readRDS("models/model_3_4.RDS")
```

```{r}
summary(model_3_4)
```

```{r}
data_3_4 <- bind_cols(data[, c("county", "year", "geometry")], fitted.values = model_3_4$summary.fitted.values$mean)

plot_rr_years(d = data_3_4, tt = "Bernardinelli model")
```

# Geostatistical data

Throughout this section, we will assume that \begin{align*}
    y_i = \mu + \zeta(x_i) + \varepsilon_i, ~\forall i,
\end{align*} where $y = (y_1, \cdots, y_n)$ are the observations at $x = (x_1, \cdots, x_n)$, $\mu$ is the common mean, $\zeta(x)$ is a zero-mean stationary and isotropic Gaussian random field, and $\varepsilon \overset{\text{i.i.d.}}{\sim}\text{Normal}(0, \sigma^2_{\varepsilon})$. In particular, $\zeta(x)$ has a [Matérn structure](https://becarioprecario.bitbucket.io/spde-gitbook/ch-intro.html#sec:matern).

Other more sophisticated models are described in [Advanced Spatial Modeling with Stochastic Partial Differential Equations Using R and INLA](https://becarioprecario.bitbucket.io/spde-gitbook/index.html) ([Krainski et al., 2019](https://becarioprecario.bitbucket.io/spde-gitbook/index.html)).

For fitting such a model, we can rely on a stochastic partial differential equation (SPDE) approach. As showed in [Whittle](.) ([1963](.)), a Gaussian random field with Matérn covariance structure can be expressed as a solution of the following SPDE \begin{align*}
    (\kappa^2 - \Delta)^{\alpha/2}(\tau S(x)) = \mathcal{W}(x),
\end{align*} where $\Delta$ is the Laplacian, $\mathcal{W}(s)$ is a Gaussian white-noise random process, $\alpha$ controls the smoothness of the random field, $\tau$ controls the variance, and $\kappa$ is a scale parameter.

Based on this result, [Lindgren et al.](https://academic.oup.com/jrsssb/article/73/4/423/7034732) ([2011](https://academic.oup.com/jrsssb/article/73/4/423/7034732)) proposed a new procedure for representing a Gaussian random field with Matérn covariance as a Gaussian Markov Random Field (GMRF). They did this by expressing a solution of the SPDE using the Finite Element Method (FEM).

The smoothness parameter $\nu$ of the Matérn covariance structure is related to $\alpha$ in the SPDE formulation through $\alpha = \nu + (d / 2)$, where $d$ (typically $d = 2$) is dimension. In `INLA`, $0 \leq \alpha < 2$.

## PM2.5

We will analyze the particulate matter 2.5 (PM2.5), measured in $\mu \text{g} / \text{m}^3)$ levels in the United States (USA) in 2022. The data is the same as the one analyzed in [Amaral et al.](https://link.springer.com/article/10.1007/s13253-023-00571-0) ([2023](https://link.springer.com/article/10.1007/s13253-023-00571-0)). The data can be downloaded from [here](https://github.com/avramaral/INLA_tutorial/raw/main/data/example_4/example_4_data.zip).

```{r}
data_USA <- readRDS(file = "data/example_4/data_USA.rds")
USA <- readRDS(file = "data/example_4/USA_filtered.rds")

head(data_USA)

ggplot() + 
  geom_sf(data = USA, fill = "white") +
  geom_sf(data = data_USA, aes(fill = mean), color = "black", size = 3, shape = 21) +
  scale_fill_gradientn(name = "PM2.5 level", colors = pal) + 
  labs(x = "", y = "", title = "") +
  custom_theme + 
  theme(axis.text.x = element_blank(),
        axis.text.y = element_blank(),
        axis.ticks  = element_blank())
```

Now, let us pre-process the data.

```{r}
# Boundary coordinates
USA_coor <- sf::st_coordinates(USA)
USA_coor <- matrix(c(USA_coor[, 1], USA_coor[, 2]), ncol = 2)
colnames(USA_coor) <- c("lon", "lat")
head(USA_coor)

# Points coordinates
data_coor <- sf::st_coordinates(data_USA)
data_USA  <- bind_cols(data_USA, as_tibble(data_coor))
data_USA  <- data_USA %>% rename(lon = X, lat = Y) %>% dplyr::select(mean, sd, lon, lat, geometry)
head(data_USA)
```

And create the mesh with the `inla.mesh.2d()` function.

```{r}
# `max.edge`: the largest allowed triangle edge length. One or two values.
# `offset`: the automatic extension distance. One or two values, for an inner and an optional outer extension.
mesh <- inla.mesh.2d(loc.domain = USA_coor, max.edge = c(300, 3000), offset = c(300, 1500))

mesh$n # Number of nodes

# Plot `mesh`
{
  plot(mesh)
  plot(USA$geometry, lwd = 2, border = "red", add = TRUE)
  points(data_USA$lon, data_USA$lat, pch = 1, col = "green")
}
```

We can build the SPDE model using one of the following functions: `inla.spde2.matern()` or `inla.spde2.pcmatern`. Notice that the parameterization of these two models is different, such that the latter accepts PC priors.

```{r}
# Building the SPDE model
# alpha = nu + d / 2 = 1 + 1 = 2, for nu = 1

# Flexible parameterization (with a default one)
# spde <- inla.spde2.matern(mesh = mesh, alpha = 2)
# Parameterization for PC priors
spde <- inla.spde2.pcmatern(mesh = mesh, 
                            alpha = 2,
                            prior.range = c(1e3, 0.90), # P(range < 1e3) = 0.90
                            prior.sigma = c(1.0, 0.01)) # P(sigma > 1.0) = 0.01
```

Now, we can create an `indxs` object for the SPDE model, as well as the projection matrix. The projection matrix depends on the `mesh` and `data_coor` and is used to project the Gaussian random field (GRF) from the observations to the triangulation vertices.

```{r}
# Indices
indxs <- inla.spde.make.index("s", spde$n.spde)
# Projection matrix
A <- inla.spde.make.A(mesh = mesh, loc = data_coor)
dim(A)
```

To make prediction, we have to define the set of coordinates where we want to estimate the process. Given location boundaries and the desired resolution, the function `create_prediction_grid()` does the job.

```{r}
create_prediction_grid <- function (loc, resolution = 25, ...) {
  pts_bdy <- loc$geometry[[1]][[1]]
  pts_bdy_x <- range(pts_bdy[, 1])
  pts_bdy_y <- range(pts_bdy[, 2])
  coord_pred <- expand.grid(x = seq(pts_bdy_x[1], pts_bdy_x[2], by = resolution), y = seq(pts_bdy_y[1], pts_bdy_y[2], by = resolution))
  coordinates(coord_pred) <- ~ x + y
  
  xx <- as(st_as_sf(coord_pred), "sf");   st_crs(xx) <- st_crs(loc$geometry)
  yy <- as(st_as_sf(loc$geometry), "sf"); st_crs(yy) <- st_crs(loc$geometry)
  
  # Compute intersection between grid and `loc` borders
  pppts <- st_intersection(x = xx, y = yy)
  
  coord_pred <- matrix(data = NA, nrow = length(pppts$geometry), ncol = 2)
  colnames(coord_pred) <- c("x", "y")
  for (p in 1:length(pppts$geometry)) { coord_pred[p, ] <- sf::st_coordinates(pppts$geometry[[p]]) }
  coord_pred <- data.frame(x = coord_pred[, 1], y = coord_pred[, 2])
  
  as.matrix(coord_pred)
}
```

```{r, eval = F}
coord_pred <- create_prediction_grid(USA)
```

```{r, echo = F}
coord_pred <- readRDS(file = "data/example_4/coord_pred.RDS")
```

```{r}
{
  coord_pred_cp <- as.data.frame(coord_pred)
  coordinates(coord_pred_cp) <- ~ x + y
  plot(coord_pred_cp)
}
```

Similar to before, we must create a projection matrix for the locations where we want to make prediction.

```{r}
# Projection matrix for prediction
Ap <- inla.spde.make.A(mesh = mesh, loc = coord_pred)
```

Lastly, we can organize the data in stacks.

```{r}
# Create stacks
# Stack for estimation 
stk_e <- inla.stack(tag = "est",
                    data = list(y = data_USA$mean),
                    A = list(1, A),
                    effects = list(data.frame(b0 = rep(1, nrow(data_USA))), s = indxs))

# Stack for prediction
stk_p <- inla.stack(tag = "pred",
                    data = list(y = NA),
                    A = list(1, Ap),
                    effects = list(data.frame(b0 = rep(1, nrow(coord_pred))), s = indxs))

# Full stack
stk_full <- inla.stack(stk_e, stk_p)
```

After all these steps, we can finally fit the model.

```{r}
formula_1 <- y ~ 0 + b0 + f(s, model = spde)
```

```{r, eval = FALSE}
model_4_1 <- inla(formula = formula_1,
                  family  = "gaussian", 
                  data = inla.stack.data(stk_full), 
                  control.predictor = list(compute = TRUE,
                                           A = inla.stack.A(stk_full))) # Matrix of predictors
```

```{r, echo = F}
model_4_1 <- readRDS(file = "models/model_4_1.RDS")
```

```{r}
summary(model_4_1)
```

As we did in the "Multiple linear regression" example, we can work with the posterior marginals.

```{r}
ss <- inla.tmarginal(fun = function(x) { 1 / sqrt(x) }, marginal = model_4_1$marginals.hyperpar$`Precision for the Gaussian observations`)

ggplot(data.frame(inla.smarginal(ss))) +
  geom_line(aes(x, y)) +
  labs(x = "", y = "", title = "Posterior of the standard deviation for the observations") + 
  custom_theme

ggplot(data.frame(inla.smarginal(model_4_1$marginals.hyperpar$`Range for s`))) +
  geom_line(aes(x, y)) +
  labs(x = "", y = "", title = "Posterior of the range in the Matérn model") + 
  custom_theme

ggplot(data.frame(inla.smarginal(model_4_1$marginals.hyperpar$`Stdev for s`))) +
  geom_line(aes(x, y)) +
  labs(x = "", y = "", title = "Posterior of the standard deviation in the Matérn model") + 
  custom_theme
```

**Remark:** According to [this parameterization](https://becarioprecario.bitbucket.io/spde-gitbook/ch-intro.html#sec:matern), the scale parameter $\kappa > 0$ can be expressed by $\frac{\sqrt{8\nu}}{\text{range}}$, where $\nu$ is the smoothness parameter.

### Prediction

Now, let us retrieve the predicted values.

```{r}
# Fitted values and prediction

# Indices for the predicted observations
idxs_pred <- inla.stack.index(stk_full, tag = "pred")$data

pred_mm <- as.data.frame(cbind(coord_pred, model_4_1$summary.fitted.values[idxs_pred, "mean"]))
pred_ll <- as.data.frame(cbind(coord_pred, model_4_1$summary.fitted.values[idxs_pred, "0.025quant"]))
pred_uu <- as.data.frame(cbind(coord_pred, model_4_1$summary.fitted.values[idxs_pred, "0.975quant"]))

# E.g.,
pred_mm %>% as_tibble() %>% rename(posterior_mean = V3) %>% head() 
```

To plot the posterior mean and percentiles, we will use the function `plot_pred_USA()`.

```{r}
plot_pred_USA <- function (fitted_values, USA, r, tt = "", should_round = TRUE, ...) {
  
  coordinates(fitted_values) <- ~ x + y
  gridded(fitted_values) <- TRUE
  fitted_values <- raster(fitted_values)
  crs(fitted_values) <- "+init=epsg:6345 +units=km +no_defs"
  
  fitted_values    <- as(fitted_values, "SpatialPixelsDataFrame")
  fitted_values_df <- as.data.frame(fitted_values)
  colnames(fitted_values_df) <- c("pred", "x", "y")
  
  if (should_round) {
    breaks <- seq(floor(r[1]), ceiling(r[2]), length.out = 5)
  } else {
    breaks <- seq(r[1], r[2], length.out = 5)
  }
  
  pal <- c("#00008FFF", "#0000F2FF", "#0063FFFF", "#00D4FFFF", "#46FFB8FF", "#B8FF46FF", "#FFD400FF", "#FF6300FF", "#F00000FF", "#800000FF")

  
  pp <- ggplot() +
    geom_tile(data = fitted_values_df, mapping = aes(x = x, y = y, fill = pred)) + 
    geom_sf(data = USA, color = "black", fill = NA, lwd = 0.5) +
    scale_fill_gradientn(name = "PM2.5", colors = pal, breaks = breaks, limits = c(breaks[1], tail(breaks, 1))) + 
    labs(x = "", y = "", title = tt) + 
    custom_theme + 
    theme(axis.text.x = element_blank(),
          axis.text.y = element_blank(),
          axis.ticks  = element_blank())
  
  pp
} 
```

The `patchwork` package allows us to combine plots.

```{r, warning = FALSE}
r_mm <- pred_mm$V3 %>% range()
r_ll <- pred_ll$V3 %>% range()
r_uu <- pred_uu$V3 %>% range()
r <- c(min(r_mm[1], r_ll[1], r_uu[1]), max(r_mm[2], r_ll[2], r_uu[2]))

pp_mm <- plot_pred_USA(fitted_values = pred_mm, USA = USA, r = r, tt = "Mean")
pp_ll <- plot_pred_USA(fitted_values = pred_ll, USA = USA, r = r, tt = "2.5th")
pp_uu <- plot_pred_USA(fitted_values = pred_uu, USA = USA, r = r, tt = "97.5th")

(pp_ll + pp_mm + pp_uu) + plot_layout(guides = "collect") & theme(legend.position = "right")
```

# Point processes

In this section, we will see how to model a log-Gaussian Cox process (LGCP) using `INLA`. However, one can refer to [this tutorial](https://avramaral.github.io/PP_inference/) for a more detailed explanation on how to make inference in point processes (also using `INLA`).

Recall that

> A Cox process can be seen as a doubly stochastic process. $\xi$ is a Cox process driven by $\Lambda(x)$ if
>
> 1.  $\{\Lambda(x); x \in \mathcal{D}\}$ is a non-negative valued stochastic process.
>
> 2.  Conditional on $\{\Lambda(x) = \lambda(x); \mathbf{x} \in \mathcal{D}\}$, $\xi$ is a Poisson process with intensity function $\lambda(x)$.

A particular case of a Cox process, named **log-Gaussian Cox process**, can be constructed by setting $\log(\Lambda(x)) = \mu^{\star}(x) + \zeta(x)$, such that $\mu(x) = \exp(\mu^{\star}(x))$ is possibly interpreted as the mean structure of $\Lambda(x)$, and $\zeta(x)$ is a stationary Gaussian process, such that $\mathbb{E}(\zeta(x)) = -\sigma^2/2$, $\forall x$, and $\text{Cov}(\zeta(x_1), \zeta(x_2)) = \phi(h) = \sigma^2 \rho(h)$, where $h$ is the distance between $x_1$ and $x_2$, and $\sigma^2$ is the variance of $\zeta(x)$.

------------------------------------------------------------------------

As we can see [here](https://avramaral.github.io/PP_inference/#fitting-a-lgcp-with-r-inla), a way to fit a LGCP with `INLA` is placing a regular grid, given by cells $\{c_{ij}\}_{ij}$, on top of the spatial domain, such that $\int_{c_{i,j}}\Lambda(x)dx \approx |c_{i,j}|\Lambda(x)$, where $|\cdot|$ denotes the area.

Thus, conditional on $\zeta(x)$, the number of locations in the grid cell $c_{ij}$, $\forall i, j$, are independent and Poisson distributed, such that \begin{align*}
\mathcal{N}(c_{ij})|\zeta(x) &\sim \text{Poisson}(|c_{ij}| \Lambda(x_{ij})), \forall i, j, \\
\log(\Lambda(x_{ij})) &= \alpha + \cdots + \zeta(x_{ij})
\end{align*} where $\zeta(x)$ is a Gaussian field (e.g., `matern2d`).

However, [Simpson et al.](https://arxiv.org/abs/1111.0641)([2016](https://arxiv.org/abs/1111.0641)) proposed a method to make inference on LGCP while still considering the exact observation locations---instead of aggregating them over a regular grid and fitting a counting model for each cell.

The main idea consists of an approximation of the likelihood by fitting a Poisson model on an augmented data set made of the observation locations and integration points from the corresponding SPDE mesh (`1` for the observed points and `0` for the dummy variables, such that all have associated "expected values" `e`). The expected number of events in the integration points is defined to be proportional to the area around the node (the areas of the polygons in the dual mesh)

**Remark:** The main reference for this section (and the next one) is [Chapter 4: Point processes and preferential sampling](https://becarioprecario.bitbucket.io/spde-gitbook/ch-lcox.html).

## PM2.5 stations

Although the problem of estimating PM2.5 depends, obviously, on the observed pollution levels, we will ignore it for now and focus on the sampling process, i.e., the locations where the monitoring stations were placed and treat them as a realization of a random process. In particular, a LGCP.

```{r}
head(data_USA)

ggplot() + 
  geom_sf(data = USA, fill = "white") +
  geom_sf(data = data_USA, color = "black", size = 3, shape = 3) +
  labs(x = "", y = "", title = "") +
  custom_theme + 
  theme(axis.text.x = element_blank(),
        axis.text.y = element_blank(),
        axis.ticks  = element_blank())
```

We will also use the data pre-processed in the previous section.

```{r}
head(data_coor)
head(USA_coor)
```

Let us start with the mesh and the SPDE model. For the former, we will use the same object (`mesh`) we created for the previous section, for the latter, we may want to use different priors.

```{r}
plot(mesh)

spde <- inla.spde2.pcmatern(mesh = mesh, 
                            alpha = 2,
                            prior.range = c(1e3, 0.90), # P(range < 1e3) = 0.90
                            prior.sigma = c(1.0, 0.01)) # P(sigma > 1.0) = 0.01
```

We can then built the dual mesh using the function `inla.dual.mesh()` (extracted from [Advanced Spatial Modeling with Stochastic Partial Differential Equations Using R and INLA](https://becarioprecario.bitbucket.io/spde-gitbook/index.html)).

```{r}
inla.dual.mesh <- function (mesh, ...) {
  if (mesh$manifold == "R2") {
    ce <- t(sapply(1:nrow(mesh$graph$tv), function (i) { colMeans(mesh$loc[mesh$graph$tv[i, ], 1:2]) }))
    library("parallel")
    pls <- mclapply(1:mesh$n, function (i) {
      p <- unique(Reduce("rbind", lapply(1:3, function (k) {
        j <- which(mesh$graph$tv[,k] == i)
        if (length(j) > 0) {
          return(rbind(ce[j, , drop = FALSE],
                       cbind(mesh$loc[mesh$graph$tv[j, k], 1] + mesh$loc[mesh$graph$tv[j, c(2:3, 1)[k]], 1], 
                             mesh$loc[mesh$graph$tv[j, k], 2] + mesh$loc[mesh$graph$tv[j, c(2:3, 1)[k]], 2]) / 2))
        } else {
          return(ce[j, , drop = FALSE])
        }
      })))
      j1 <- which(mesh$segm$bnd$idx[, 1] == i)
      j2 <- which(mesh$segm$bnd$idx[, 2] == i)
      if ((length(j1) > 0) | (length(j2) > 0)) {
        p <- unique(rbind(mesh$loc[i, 1:2], p,
                          mesh$loc[mesh$segm$bnd$idx[j1, 1], 1:2] / 2 + mesh$loc[mesh$segm$bnd$idx[j1, 2], 1:2] / 2, 
                          mesh$loc[mesh$segm$bnd$idx[j2, 1], 1:2] / 2 + mesh$loc[mesh$segm$bnd$idx[j2, 2], 1:2] / 2))
        yy <- p[, 2] - mean(p[, 2]) / 2 - mesh$loc[i, 2] / 2
        xx <- p[, 1] - mean(p[, 1]) / 2 - mesh$loc[i, 1] / 2
      } else {
        yy <- p[, 2] - mesh$loc[i, 2]
        xx <- p[, 1] - mesh$loc[i, 1]
      }
      Polygon(p[order(atan2(yy, xx)), ])
    })
    return(SpatialPolygons(lapply(1:mesh$n, function (i) { Polygons(list(pls[[i]]), i)} )))
  }
  else { stop("It only works for R2.") }
}
```

```{r}
dual_mesh <- inla.dual.mesh(mesh)
```

Now, we can compute the weights (`wgt`) based on the dual mesh, such that the regions outside of the location boundaries have weight `0`.

```{r, eval = F}
USA_poly <- SpatialPolygons(list(Polygons(list(Polygon(USA_coor)), ID = "1")))
wgt <- sapply(1:length(dual_mesh), function (i) {
  if (gIntersects(dual_mesh[i, ], USA_poly)) {
    return(gArea(gIntersection(dual_mesh[i, ], USA_poly)))
  } else {
    return(0)
  }
})
```

```{r, echo = F}
wgt <- readRDS(file = "data/example_5/wgt.RDS")
```

```{r}
hist(wgt, main = "")

# Plotting dual mesh color coded based on the `wgt`
{
  plot(mesh$loc, asp = 1, col = (wgt == 0) + 1, pch = 19, xlab = "", ylab = "", axes = F, cex = 0.6) 
  plot(mesh, add = TRUE)
  plot(dual_mesh, add = TRUE)
  plot(USA$geometry, add = TRUE, border = "green", lwd = 2)
}
```

As before, we must construct the projection matrices and format the data in the "right" way (i.e., in stacks). For prediction, we use assume the same `coord_pred` as in the previous section.

```{r}
head(coord_pred)

n_vtx <- mesh$n
n_pts <- nrow(data_USA)
n_pts_pred <- nrow(coord_pred)

# Indices
indxs <- inla.spde.make.index("s", spde$n.spde)

# augmented data: `0` for the mesh nodes and `1` for the observations
y_pp <- rep(0:1, c(n_vtx, n_pts))
# Exposure vector
e_pp <- c(wgt, rep(0, n_pts)) 
# Projection matrix (in two parts)
# (1) For the integration points, this is just a diagonal matrix--as these locations are just the mesh vertices
imat <- Diagonal(n_vtx, rep(1, n_vtx))
# (2) For the observed points, the projection matrix is defined with `inla.spde.make.A`
ymat <- inla.spde.make.A(mesh, data_coor)
# (1) + (2)
A_pp <- rbind(imat, ymat)

# Prediction
A_pp_p <- inla.spde.make.A(mesh = mesh, loc = coord_pred) # Projection matrix for the prediction points

# Create stacks
# Stack for estimation 
stk_pp_e <- inla.stack(tag = "est_pp",
                       data = list(y = y_pp, e = e_pp),
                       A = list(1, A_pp),
                       effects = list(alpha_pp = rep(1, n_vtx + n_pts), s = indxs))

stk_pp_p <- inla.stack(tag = "pred_pp",
                       data = list(y = rep(NA, n_pts_pred), e = rep(1, n_pts_pred)),
                       A = list(1, A_pp_p),
                       effects = list(alpha_pp = rep(1, n_pts_pred), s = indxs))

# Full stack
stk_full_pp <- inla.stack(stk_pp_e, stk_pp_p)
```

Let us fit the model. Note that we are using a Poisson likelihood.

```{r, eval = F}
formula_1 <- y ~ 0 + alpha_pp + f(s, model = spde)

model_5_1 <- inla(formula = formula_1,
                  family  = "poisson", 
                  E = inla.stack.data(stk_full_pp)$e,
                  data = inla.stack.data(stk_full_pp), 
                  control.predictor = list(link = 1,
                                           compute = TRUE,
                                           A = inla.stack.A(stk_full_pp)))
```

```{r, echo = F}
model_5_1 <- readRDS(file = "models/model_5_1.RDS")
```

```{r}
summary(model_5_1)
```

### Prediction

Now, let us retrieve the predicted values for the intensity function and plot them.

```{r}
idx_pp <- inla.stack.index(stk_full_pp, tag = "pred_pp")$data

pred_pp_mm <- as.data.frame(cbind(coord_pred, model_5_1$summary.fitted.values[idx_pp, "mean"]))
pred_pp_ll <- as.data.frame(cbind(coord_pred, model_5_1$summary.fitted.values[idx_pp, "0.025quant"]))
pred_pp_uu <- as.data.frame(cbind(coord_pred, model_5_1$summary.fitted.values[idx_pp, "0.975quant"]))

# Expected number of observations
sum(pred_pp_mm$V3 * (25 ** 2))

r_mm <- pred_pp_mm$V3 %>% range()
r_ll <- pred_pp_ll$V3 %>% range()
r_uu <- pred_pp_uu$V3 %>% range()
r <- c(min(r_mm[1], r_ll[1], r_uu[1]), max(r_mm[2], r_ll[2], r_uu[2]))

pp_pp_mm <- plot_pred_USA(fitted_values = pred_pp_mm, USA = USA, r = r, tt = "Mean",   should_round = FALSE)
pp_pp_ll <- plot_pred_USA(fitted_values = pred_pp_ll, USA = USA, r = r, tt = "2.5th",  should_round = FALSE)
pp_pp_uu <- plot_pred_USA(fitted_values = pred_pp_uu, USA = USA, r = r, tt = "97.5th", should_round = FALSE)

(pp_pp_ll + pp_pp_mm + pp_pp_uu) + plot_layout(guides = "collect") & theme(legend.position = "right")
```

# Preferential sampling (PS)

At this point, we can implement a more sophisticated model for PM2.5. We will assume that the sampling process (modeled in the previous section) is **not** independent of the latent field. In our problem, this is equivalent to stating that the locations where the monitoring stations were placed are somehow influenced by the pollution levels at those sites. This is known as **preferential sampling** ([Diggle et al., 2010](https://doi.org/10.1111/j.1467-9876.2009.00701.x)).

Formally, \begin{align*}
    y_i &= \mu + \zeta(x_i) + \varepsilon_i, ~\forall i, \\
    \xi|\zeta(x) &\sim \text{PPP}(\Lambda(x)) \\  
    \log(\Lambda(x)) &= \alpha + \cdots + \gamma \cdot \zeta(x)
\end{align*} where $\gamma \in \mathbb{R}$ is the "degree of preferentiality", and all other quantities are defined as before.

In `INLA` we can fit this model using multiple likelihoods and `copy` of the random effects.

## PM2.5

To fit the model, we will re-use many objects we already defined in previous sections.

```{r}
# The following objects will be re-used
# `data_USA` 
# `USA`
# `data_coor`
# `USA_coor`
# `mesh`
# `dual_mesh`
# `wgt`
# `coord_pred`

ggplot() + 
  geom_sf(data = USA, fill = "white") +
  geom_sf(data = data_USA, aes(fill = mean), color = "black", size = 3, shape = 21) +
  scale_fill_gradientn(name = "PM2.5 level", colors = pal) + 
  labs(x = "", y = "", title = "") +
  custom_theme + 
  theme(axis.text.x = element_blank(),
        axis.text.y = element_blank(),
        axis.ticks  = element_blank())
```

As the SPDE model may have different priors, we will define it again.

```{r}
spde <- inla.spde2.pcmatern(mesh = mesh, 
                            alpha = 2,
                            prior.range = c(1e3, 0.90), # P(range < 1e3) = 0.90
                            prior.sigma = c(1.0, 0.01)) # P(sigma > 1.0) = 0.01
```

Similar to before, let us construct the projection matrices and format the data into stacks.

```{r}
n_vtx <- mesh$n
n_pts <- nrow(data_USA)
n_pts_pred <- nrow(coord_pred)

indxs <- inla.spde.make.index("s", spde$n.spde)
indxv <- inla.spde.make.index("v", spde$n.spde)

y_pp <- rep(0:1, c(n_vtx, n_pts))
e_pp <- c(wgt, rep(0, n_pts)) 
imat <- Diagonal(n_vtx, rep(1, n_vtx))
ymat <- inla.spde.make.A(mesh, data_coor)
A_pp <- rbind(imat, ymat)
# Prediction
A_pp_p <- inla.spde.make.A(mesh = mesh, loc = coord_pred) 
```

The stacks will have two columns for the `data` component, one for each likelihood. Besides, notice that we have stacks for the `gaussian` component (`stk_y_X`) and for the `poisson` component (`stk_pp_X`)—both for estimation and prediction. At the end, all of them are combined into one object (`stk_full_pp_y`).

```{r}
# Create stacks

# `data` has two columns, one for each likelihood

stk_y_e <- inla.stack(tag = "est_y",
                      data = list(y = cbind(data_USA$mean, NA), e = rep(NA, n_pts)),
                      A = list(1, ymat),
                      effects = list(mu = rep(1, n_pts), s = indxs)) 

stk_y_p <- inla.stack(tag = "pred_y",
                      data = list(y = cbind(rep(NA, n_pts_pred), NA), e = rep(NA, n_pts_pred)),
                      A = list(1, A_pp_p),
                      effects = list(mu = rep(1, n_pts_pred), s = indxs)) 

stk_pp_e <- inla.stack(tag = "est_pp",
                       data = list(y = cbind(NA, y_pp), e = e_pp),
                       A = list(1, A_pp),
                       effects = list(alpha_pp = rep(1, n_vtx + n_pts), v = indxv))

stk_pp_p <- inla.stack(tag = "pred_pp",
                       data = list(y = cbind(NA, rep(NA, n_pts_pred)), e = rep(1, n_pts_pred)),
                       A = list(1, A_pp_p),
                       effects = list(alpha_pp = rep(1, n_pts_pred), v = indxv))

# Full stack
stk_full_pp_y <- inla.stack(stk_y_e, stk_y_p, stk_pp_e, stk_pp_p)
```

Now, let us fit the model.

To do so, notice that we are "copying" a random effect. The `copy` feature allows for the inclusion of a shared term among several linear predictors. Besides, we are assigning a $\text{Normal}(0, 1)$ prior for $\gamma$.

```{r}
re_prior <- list(prior = "gaussian", param = c(0, 1))

# `fixed = FALSE` allows the copy coefficient to be estimated
formula_1 <- y ~ 0 + mu + alpha_pp + f(s, model = spde) + f(v, copy = "s", fixed = FALSE, hyper = list(beta = re_prior))
```

```{r, eval = F}
model_6_1 <- inla(formula = formula_1,
                  family  = c("gaussian", "poisson"), 
                  E = inla.stack.data(stk_full_pp_y)$e,
                  data = inla.stack.data(stk_full_pp_y), 
                  control.predictor = list(link = rep(c(1, 2), c((n_pts + n_pts_pred), (n_vtx + n_pts + n_pts_pred))),
                                           compute = TRUE,
                                           A = inla.stack.A(stk_full_pp_y)))
```

```{r, echo = F}
model_6_1 <- readRDS(file = "models/model_6_1.RDS")
```

```{r}
summary(model_6_1)

ggplot(data.frame(inla.smarginal(model_6_1$marginals.hyperpar$`Beta for v`))) +
  geom_line(aes(x, y)) +
  labs(x = "", y = "", title = "Posterior of the degree of preferentiality (gamma)") + 
  custom_theme
```

### Prediction

Similar to before, we can finally retrieve the fitted values—both for the response and intensity processes.

```{r}
## Response

idx_y  <- inla.stack.index(stk_full_pp_y, tag = "pred_y" )$data

pred_y_mm  <- as.data.frame(cbind(coord_pred, model_6_1$summary.fitted.values[idx_y,  "mean"]))
pred_y_ll  <- as.data.frame(cbind(coord_pred, model_6_1$summary.fitted.values[idx_y,  "0.025quant"]))
pred_y_uu  <- as.data.frame(cbind(coord_pred, model_6_1$summary.fitted.values[idx_y,  "0.975quant"]))

r_mm <- pred_y_mm$V3 %>% range()
r_ll <- pred_y_ll$V3 %>% range()
r_uu <- pred_y_uu$V3 %>% range()
r <- c(min(r_mm[1], r_ll[1], r_uu[1]), max(r_mm[2], r_ll[2], r_uu[2]))

pp_y_mm <- plot_pred_USA(fitted_values = pred_y_mm, USA = USA, r = r, tt = "Mean")
pp_y_ll <- plot_pred_USA(fitted_values = pred_y_ll, USA = USA, r = r, tt = "2.5th")
pp_y_uu <- plot_pred_USA(fitted_values = pred_y_uu, USA = USA, r = r, tt = "97.5th")

(pp_y_ll + pp_y_mm + pp_y_uu) + plot_layout(guides = "collect") & theme(legend.position = "right")

## Intensity process

idx_pp <- inla.stack.index(stk_full_pp_y, tag = "pred_pp")$data

pred_pp_mm <- as.data.frame(cbind(coord_pred, model_6_1$summary.fitted.values[idx_pp, "mean"]))
pred_pp_ll <- as.data.frame(cbind(coord_pred, model_6_1$summary.fitted.values[idx_pp, "0.025quant"]))
pred_pp_uu <- as.data.frame(cbind(coord_pred, model_6_1$summary.fitted.values[idx_pp, "0.975quant"]))

# Expected number of observations
sum(pred_pp_mm$V3 * (25 ** 2))

r_mm <- pred_pp_mm$V3 %>% range()
r_ll <- pred_pp_ll$V3 %>% range()
r_uu <- pred_pp_uu$V3 %>% range()
r <- c(min(r_mm[1], r_ll[1], r_uu[1]), max(r_mm[2], r_ll[2], r_uu[2]))

pp_pp_mm <- plot_pred_USA(fitted_values = pred_pp_mm, USA = USA, r = r, tt = "Mean",   should_round = FALSE)
pp_pp_ll <- plot_pred_USA(fitted_values = pred_pp_ll, USA = USA, r = r, tt = "2.5th",  should_round = FALSE)
pp_pp_uu <- plot_pred_USA(fitted_values = pred_pp_uu, USA = USA, r = r, tt = "97.5th", should_round = FALSE)

(pp_pp_ll + pp_pp_mm + pp_pp_uu) + plot_layout(guides = "collect") & theme(legend.position = "right")
```

# One more thing (Spat. varying PS)

Expanding upon the PS model presented in the previous section, it may be desirable to enable the degree of preferentiality to vary across space. This could be especially beneficial when analyzing extensive regions, as the strength of dependence between the latent field and the sampling process is unlikely to remain constant across the spatial domain. This approach was originally proposed by [Amaral et al.](https://doi.org/10.1007/s13253-023-00571-0) ([2023](https://doi.org/10.1007/s13253-023-00571-0)) with code available [here](https://github.com/avramaral/preferential_sampling/).

In particular, \begin{align*} 
    y_i &= \mu + \zeta(x_i) + \epsilon_i, \forall i \\ 
  \xi|\zeta(x) &\sim \text{PPP}(\Lambda(x)) \\
  \log(\lambda(x)) &= \alpha + \cdots + \gamma(x) \cdot \zeta(x) \\
  \gamma(x) &= \sum_{k = 1}^{K}\beta_k \phi_k(x), \text{ s.t. } \beta_k \overset{\text{i.i.d.}}{\sim} \text{Normal}(0, \sigma^2_{\beta}), \forall k.
\end{align*} From the above formulation, notice that $\gamma(x)$ is approximated by a combination of unknown coefficients $\{\beta_k\}_k$ and basis functions $\{\phi_k(x)\}_k$. All remaining quantities are defined as before.

The type, number, and location of such basis functions are problem-dependent and [Amaral et al.](https://doi.org/10.1007/s13253-023-00571-0) ([2023](https://doi.org/10.1007/s13253-023-00571-0)) provide guidelines on how to set these aspects of the model.

For this tutorial, we will use "radial basis" built using a compactly supported Wendland ([Wendland, 2015](https://doi.org/10.1007/BF02123482)) function defined in space.

> Radial Basis (Wendland): let $h_k = \left(||x - a_k|| / b_k\right)$, such that $b_k \in \mathbb{R}$ and $a_k \in \mathbb{R}^2$, $\forall k$. Under this setting, $a_k$ is also known as a "node point," and, sometimes, $b_k$ can also be referred to as "bandwidth." Then, $$ \phi_k(x) = \begin{cases} (1 - h_k)^6 (35h_k^2 + 18h_k + 3) / 3 & h_k \in [0, 1] \\ 0   & \text{otherwise.} \end{cases} $$

## PM2.5

Again, to fit the model, we will re-use objects we already defined in previous sections.

```{r}
# The following objects will be re-used
# `data_USA` 
# `USA`
# `data_coor`
# `USA_coor`
# `mesh`
# `dual_mesh`
# `wgt`
# `coord_pred`

ggplot() + 
  geom_sf(data = USA, fill = "white") +
  geom_sf(data = data_USA, aes(fill = mean), color = "black", size = 3, shape = 21) +
  scale_fill_gradientn(name = "PM2.5 level", colors = pal) + 
  labs(x = "", y = "", title = "") +
  custom_theme + 
  theme(axis.text.x = element_blank(),
        axis.text.y = element_blank(),
        axis.ticks  = element_blank())
```

As the SPDE model may have different priors, we will define it again.

```{r}
spde <- inla.spde2.pcmatern(mesh = mesh, 
                            alpha = 2,
                            prior.range = c(1e3, 0.90), # P(range < 1e3) = 0.90
                            prior.sigma = c(1.0, 0.01)) # P(sigma > 1.0) = 0.01
```

The next step is to pre-compute the basis functions on the "mesh nodes + observed locations" for estimation and on the "prediction locations" for prediction. To do so, we will use the functions `basis_functions()` and `Wendland()`.

**Remark:** user can define other smoothing kernels (e.g., `Wendland()`) and use them along with `basis_functions()`.

```{r}
# Generic function
basis_functions <- function (center_pts, loct, mesh = NULL, bandwidth, smoothing_kernel = "Wendland", ...) {
  if (is.null(mesh)) {
    all_pts <- loct
  } else {
    all_pts <- rbind(mesh$loc[, 1:2], loct)
  }
  
  f <- get(smoothing_kernel)
  b <- list()
  n_basis_functions <- nrow(center_pts)
  
  for (i in 1:n_basis_functions) { b[[i]] <- f(x = all_pts[, 1], y = all_pts[, 2], center_x = center_pts[i, 1], center_y = center_pts[i, 2], bandwidth = bandwidth) }
  
  b
}

# Basis functions implementations (e.g., `Wendland`)
Wendland <- function (x, y, center_x, center_y, bandwidth = 0.5, ...) {
  pts <- cbind((x - center_x), (y - center_y))
  d <- apply(X = pts, MARGIN = 1, FUN = function (x) { sqrt(sum(x ** 2)) }) / bandwidth
  k <- (1 - d) ^ 6 * (35 * d ^ 2 + 18 * d + 3) / 3 * as.numeric(d <= 1)
  k <- k / max(k)
  k
}
```

Thus, for `bandwidth = 2500` and `center_pts` defined as below, we can pre-compute the basis functions for both estimation and prediction.

```{r}
bandwidth <- 2500
smoothing_kernel <- "Wendland"
center_pts <- as.matrix(rbind(c(-2372, 4346), c(-1984, 5363), c( -918, 5143),
                              c( -518, 3817), c(  263, 4987), c(  677, 4096),
                              c( 1494, 4720), c( -198, 4291), c(-1710, 4513),
                              c(  875, 3602), c(-1202, 3742), c(-1310, 4946),
                              c( -190, 5189), c(  920, 4550), c( -233, 4800)))

bfs      <- basis_functions(center_pts = center_pts, loct = data_coor,  mesh = mesh, smoothing_kernel = smoothing_kernel, bandwidth = bandwidth)
bfs_pred <- basis_functions(center_pts = center_pts, loct = coord_pred, mesh = NULL, smoothing_kernel = smoothing_kernel, bandwidth = bandwidth)

pps <- list()
for (i in 1:nrow(center_pts)) {
  pps[[i]] <- plot_pred_USA(fitted_values = as.data.frame(cbind(coord_pred, bfs_pred[[i]])), USA = USA, r = c(0, 1), tt = i)
}

((pps[[ 1]] + pps[[ 2]] + pps[[ 3]]) / 
 (pps[[ 4]] + pps[[ 5]] + pps[[ 6]]) /
 (pps[[ 7]] + pps[[ 8]] + pps[[ 9]]) / 
 (pps[[10]] + pps[[11]] + pps[[12]]) /
 (pps[[13]] + pps[[14]] + pps[[15]])) + plot_layout(guides = "collect") & theme(legend.position = "none")
```

One more time, let us construct the projection matrices and format the data into stacks.

Notice that projection matrices for all effects in the Poisson model are obtained by multiplying column-wise a base matrix by the vector of basis function evaluated at the corresponding locations.

```{r}
n_vtx <- mesh$n
n_pts <- nrow(data_USA)
n_pts_pred <- nrow(coord_pred)

indx <- 1:spde$n.spde

y_pp <- rep(0:1, c(n_vtx, n_pts))
e_pp <- c(wgt, rep(0, n_pts)) 
imat <- Diagonal(n_vtx, rep(1, n_vtx))
ymat <- inla.spde.make.A(mesh, data_coor)

# Estimation
A_pp_base <- rbind(imat, ymat)
n_basis_functions <- nrow(center_pts)
A_pp <- list()
for (i in 1:n_basis_functions) {
  A_pp[[i]] <- A_pp_base * bfs[[i]] # Multiply each column by the basis function evaluated at the corresponding locations
}

# Prediction
A_pp_p_base <- inla.spde.make.A(mesh = mesh, loc = as.matrix(coord_pred))
A_pp_p <- list()
for (i in 1:n_basis_functions) {
  A_pp_p[[i]] <- A_pp_p_base * bfs_pred[[i]]
}
```

The stacks are defined similarly to before. However, for the Poisson model, it's important to note that effects are built dynamically and reflect the number of basis functions.

```{r}
# Create stacks

#########################
# Create a list of effects based on the number of basis function
#########################

fx_effects <- list(rep(1, n_vtx + n_pts))
rd_effects <- list()
for (i in 1:length(A_pp)) { rd_effects <- append(rd_effects, list(indx)) } 
effects <- append(fx_effects, rd_effects)
names(effects) <- c("alpha_pp", paste("v_", 1:length(A_pp), sep = ""))

effects_pred <- effects
effects_pred[[1]] <- rep(1, times = n_pts_pred)

#########################
#########################

# Stacks for `y` are similar to before; however, stacks for `pp` have different structures for `A` and `effects`

stk_y_e <- inla.stack(tag = "est_y",
                      data = list(y = cbind(data_USA$mean, NA), e = rep(NA, n_pts)),
                      A = list(1, ymat),
                      effects = list(mu = rep(1, n_pts), s = indx)) 

stk_y_p <- inla.stack(tag = "pred_y",
                      data = list(y = cbind(rep(NA, n_pts_pred), NA), e = rep(NA, n_pts_pred)),
                      A = list(1, A_pp_p_base),
                      effects = list(mu = rep(1, n_pts_pred), s = indx)) 

stk_pp_e <- inla.stack(tag = "est_pp",
                       data = list(y = cbind(NA, y_pp), e = e_pp),
                       A = append(1, A_pp),
                       effects = effects)

stk_pp_p <- inla.stack(tag = "pred_pp",
                       data = list(y = cbind(NA, rep(NA, n_pts_pred)), e = rep(1, n_pts_pred)),
                       A = append(1, A_pp_p),
                       effects = effects_pred)

# Full stack
stk_full_pp_y <- inla.stack(stk_y_e, stk_y_p, stk_pp_e, stk_pp_p)
```

Now, let us fit the model. In this case, although the formula has a similar structure to what we used for the PS model, we also build it dynamically, as it depends on the number of basis functions.

```{r}
random_effects <- paste(paste("f(v_",  1:length(A_pp), ", copy = \"s\", fixed = FALSE, hyper = list(beta = re_prior))", sep = ""), collapse = " + ")
formula_1 <- paste("y ~ 0 + mu + alpha_pp + f(s, model = spde) + ", random_effects, sep = "")
formula_1 <- eval(parse(text = formula_1))
formula_1
```

```{r, eval = FALSE}
model_7_1 <- inla(formula = formula_1,
                  family  = c("gaussian", "poisson"), 
                  E = inla.stack.data(stk_full_pp_y)$e,
                  data = inla.stack.data(stk_full_pp_y), 
                  control.predictor = list(link = rep(c(1, 2), c((n_pts + n_pts_pred), (n_vtx + n_pts + n_pts_pred))),
                                           compute = TRUE,
                                           A = inla.stack.A(stk_full_pp_y)))
```

```{r}
model_7_1 <- readRDS(file = "models/model_7_1.RDS")
```

```{r}
summary(model_7_1)
```

### Prediction

Finally, we can retrieve the (1) fitted values, (2) estimated intensity process, and (3) estimated preferentiality surface $\hat{\gamma}(x)$.

```{r}
# Fitted values and prediction

## Response

idx_y  <- inla.stack.index(stk_full_pp_y, tag = "pred_y" )$data

pred_y_mm  <- as.data.frame(cbind(coord_pred, model_7_1$summary.fitted.values[idx_y,  "mean"]))
pred_y_ll  <- as.data.frame(cbind(coord_pred, model_7_1$summary.fitted.values[idx_y,  "0.025quant"]))
pred_y_uu  <- as.data.frame(cbind(coord_pred, model_7_1$summary.fitted.values[idx_y,  "0.975quant"]))

r_mm <- pred_y_mm$V3 %>% range()
r_ll <- pred_y_ll$V3 %>% range()
r_uu <- pred_y_uu$V3 %>% range()
r <- c(min(r_mm[1], r_ll[1], r_uu[1]), max(r_mm[2], r_ll[2], r_uu[2]))

pp_y_mm <- plot_pred_USA(fitted_values = pred_y_mm, USA = USA, r = r, tt = "Mean")
pp_y_ll <- plot_pred_USA(fitted_values = pred_y_ll, USA = USA, r = r, tt = "2.5th")
pp_y_uu <- plot_pred_USA(fitted_values = pred_y_uu, USA = USA, r = r, tt = "97.5th")

(pp_y_ll + pp_y_mm + pp_y_uu) + plot_layout(guides = "collect") & theme(legend.position = "right")

## Intensity process

idx_pp <- inla.stack.index(stk_full_pp_y, tag = "pred_pp")$data

pred_pp_mm <- as.data.frame(cbind(coord_pred, model_7_1$summary.fitted.values[idx_pp, "mean"]))
pred_pp_ll <- as.data.frame(cbind(coord_pred, model_7_1$summary.fitted.values[idx_pp, "0.025quant"]))
pred_pp_uu <- as.data.frame(cbind(coord_pred, model_7_1$summary.fitted.values[idx_pp, "0.975quant"]))

# Expected number of observations
sum(pred_pp_mm$V3 * (25 ** 2))

r_mm <- pred_pp_mm$V3 %>% range()
r_ll <- pred_pp_ll$V3 %>% range()
r_uu <- pred_pp_uu$V3 %>% range()
r <- c(min(r_mm[1], r_ll[1], r_uu[1]), max(r_mm[2], r_ll[2], r_uu[2]))

pp_pp_mm <- plot_pred_USA(fitted_values = pred_pp_mm, USA = USA, r = r, tt = "Mean",   should_round = FALSE)
pp_pp_ll <- plot_pred_USA(fitted_values = pred_pp_ll, USA = USA, r = r, tt = "2.5th",  should_round = FALSE)
pp_pp_uu <- plot_pred_USA(fitted_values = pred_pp_uu, USA = USA, r = r, tt = "97.5th", should_round = FALSE)

(pp_pp_ll + pp_pp_mm + pp_pp_uu) + plot_layout(guides = "collect") & theme(legend.position = "right")
```

The estimated preferentiality surface $\hat{\gamma}(x)$ is determined based on the pre-computed basis functions $\{\phi_k(x)\}_k$ and the posterior mean of $\{\beta_k\}_k$.

```{r}
n_basis_functions
n_hyperparameter <- nrow(model_7_1$summary.hyperpar)

coeff <- model_7_1$summary.hyperpar[((n_hyperparameter - n_basis_functions + 1):n_hyperparameter), "mean"]
value <- rep(0, n_pts_pred) 
for (i in 1:n_basis_functions) {
  bfs_pred_tmp <- bfs_pred[[i]]
  value <- value + (bfs_pred_tmp * coeff[i])
}

pref_mm <- as.data.frame(cbind(coord_pred, value))

plot_pred_USA(fitted_values = pref_mm, USA = USA, r = range(pref_mm$value), tt = "Preferentiality surface", should_round = FALSE)
```
