---
title: "INLA Tutorial"
author: "[André V. Ribeiro Amaral](https://www.avramaral.com/)"
format: 
  html:
    toc: true
    html-math-method: katex
    css: styles.css
---

```{r, echo = F}
knitr::opts_knit$set(global.par = T)

options(dplyr.summarise.inform = FALSE)
```

In this tutorial, we will use the following `R` packages.

```{r, result = "hide", message = F, warning = F}
library("INLA")
library("tidyverse")
library("spatstat")
library("raster")
library("sf")

library("MASS")
library("spdep")
library("patchwork")
```

To install `INLA`, you can follow the instruction [here](https://www.r-inla.org/download-install). Alternatively,
```{r, eval = F}
install.packages("INLA", repos = c(getOption("repos"), INLA = "https://inla.r-inla-download.org/R/stable"), dep = TRUE) 
```

Along the tutorial, we will use the following common theme for the plots with `ggplot2`

```{r}
custom_theme <-  theme_bw() + theme(legend.position = "right", 
                              text = element_text(size = 14, family = "LM Roman 10"),
                              plot.title = element_text(size = 16),
                              legend.title = element_text(size = 12))
```


# INLA

In a nutshell, the integrated nested Laplace approximation (INLA) ([Rue et al., 2009](https://doi.org/10.1111/j.1467-9868.2008.00700.x)) method is used to approximate Bayesian inference in latent Gaussian models. In particular, it can be used to fit models of the form
\begin{align*}
	y_i|S(x_i), &\theta \sim \pi(y_i|S(x_i), \theta), \text{ for } i \in \{1, \cdots, n\} \\
	S(x)|\theta &\sim \text{Normal}(\mu(\theta), Q(\theta)^{-1}) \\
	\theta &\sim \pi(\theta),
\end{align*}
where $y = (y_1, \ldots, y_n)$ is the vector or observed values, $x = (x_1, \ldots, x_n)$ is a Gaussian random field, and $\theta = (\theta_1, \ldots, \theta_k)$, for some $k \in \mathbb{N}$, is a vector of hyperparameters. $\mu(\theta)$ and $Q(\theta)$ represent the mean vector and the precision matrix,
respectively.

The observed values $y_i$, $\forall i$, are assumed to have mean $\mu_i=g^{-1}(\eta_i)$, such that the linear prediction $\eta_i$ can be expressed as follows
\begin{align*}
  \eta_i = \alpha + \sum^{n_{\beta}}_{k = 1} \beta_k z_{ki} + \sum_{j = 1}^{n_f}f^{(j)}(u_{ij}) + \varepsilon_i, \forall i,
\end{align*}
where $\alpha$ is the intercept, $\{\beta_k\}_k$, are the coefficients of covariates $\{z_{ki}\}_{ki}$, and $\{f^{(j)}\}_j$ define some random effects in terms of covariates $\{u_{ij}\}_{ij}$. Lastly, $\varepsilon_i$ is an error term.

In this formulation, $\{f^{(j)}\}_j$ will be commonly defined as spatio(-temporal) structures---at least for the purposes of this tutorial.

# Multiple linear regression

Let us start with a multiple linear regression problem, so that we can get to know the `INLA` notation and how to extract the desired quantities. This example was extracted from [Bayesian Inference with INLA](https://becarioprecario.bitbucket.io/inla-gitbook/ch-INLA.html#multiple-linear-regression) ([Gómez-Rubio, 2021](https://becarioprecario.bitbucket.io/inla-gitbook/index.html)).

We will analyze the `cement` data set from the `MASS` package. `x1`, `x2`, `x3`, and `x4` represent the percentages of the four main chemical ingredients in the setting of 13 cements, and `y` measures the heat evolved. 

```{r}
summary(cement)

```

Let us add another data point for prediction.

```{r}
cement <- rbind(cement, data.frame(x1 = 24, x2 = 24, x3 = 24, x4 = 24, y  = NA))
```

And let us fit the following model
\begin{align*}
y_i = \alpha + \sum_{j = 1}^{4} \beta_j x_{ji} + \varepsilon_i, \forall i,
\end{align*}
where $\varepsilon_i\overset{\text{i.i.d.}}{\sim}\text{Normal}(0, 1/\tau)$.

```{r}
formula_1 <- y ~ 0 + 1 + x1 + x2 + x3 + x4
```

```{r, eval = F}
model_1_1 <- inla(formula = formula_1, 
                  family  = "gaussian",
                  data    = cement,
                  control.predictor = list(compute = TRUE)) # Should the marginals for the linear predictor be computed?
```

```{r, echo = F}
model_1_1 <- readRDS("models/model_1_1.RDS")
```

```{r}
summary(model_1_1)

# model_1_1$summary.fixed              # Estimated fixed effects
# model_1_1$summary.hyperpar           # Estimated hyperparameters
# model_1_1$summary.linear.predictor   # Estimated values for the linear predictor
model_1_1$summary.fitted.values[, 1:5] # Fitted values (+ prediction)
```

`INLA` has numerous functions to process the posterior marginals. For instance, `inla.smarginal()` is used to obtain a spline smoothing.

```{r}
alpha <- model_1_1$marginals.fixed[[1]]

ggplot(data.frame(inla.smarginal(alpha)), aes(x, y)) +
  geom_line() +
  labs(x = "", y = "", title = "Posterior of alpha") + 
  custom_theme
```

`inla.qmarginal()` and `inla.pmarginal()` compute the quantile and distribution function, respectively.

```{r}
qq <- inla.qmarginal(0.05, alpha)
qq

inla.pmarginal(qq, alpha)
```

`inla.dmarginal()` computes the density at particular values.

```{r}
inla.dmarginal(0, alpha)
```

`inla.tmarginal()` transforms the marginal. This is useful, e.g., to obtain the posterior distribution of the $1 / \tau$.

```{r}
sigma_2 <- inla.tmarginal(fun = function(x) { 1 / x }, marginal = model_1_1$marginals.hyperpar$`Precision for the Gaussian observations`)

ggplot(data.frame(inla.smarginal(sigma_2))) +
  geom_line(aes(x, y)) +
  labs(x = "", y = "", title = "Posterior of the variance") + 
  custom_theme
```

Complementary, we can also plot the posterior distribution of the fitted values.

```{r}
post_fitted <- model_1_1$marginals.fitted.values

post_margin <- data.frame(do.call(rbind, post_fitted))
post_margin$cement <- rep(names(post_fitted), times = sapply(post_fitted, nrow))

ggplot(post_margin) + 
  geom_line(aes(x, y)) +
  facet_wrap(~ cement, ncol = 5) +
  labs(x = "", y = "Density") +
  custom_theme
```

# Generalized linear models (for disease mapping)

Next, let us fit a generalized linear model (GLM) for a problem in disease mapping. Initially, we will ignore the underlying spatial structure, but we will consider it in the following part. This example was extracted from [Geospatial Health Data: Modeling and Visualization with R-INLA and Shiny](https://www.paulamoraga.com/book-geospatial/sec-arealdataexamplest.html) ([Moraga, 2019](https://www.paulamoraga.com/book-geospatial/index.html)).

We will model the number of cases of lung cancer in Ohio, USA, from 1968 to 1988. The data can be downloaded from [here](.).

After downloading the data, we can do the following

```{r}
# Load data
d_ohio <- read_csv(file = "data/example_2/data_ohio.csv", show_col_types = FALSE)
m_ohio <- read_sf(dsn = "data/example_2/ohio_shapefile/", layer = "fe_2007_39_county")

head(d_ohio)
plot(m_ohio$geometry)
```

For this problem, we would like to model the relative risk of cancer in a certain region. To do so, we can compute first the "standardized incidence ratio" (SIR). 

Let $Y_i$ denote the number of cases in the location $i$, then
\begin{align*}
  \text{SIR}_i = \frac{Y_i}{E_i}, \forall i,
\end{align*}
where 
\begin{align*}
  E_i = \sum_{j = 1}^{m} r_j^{(s)} \cdot n_j^{(i)},
\end{align*}
such that $r_j^{(s)}$ represents the rate in stratum $j$ in the standard population, and $n_j^{(i)}$ is the population in stratum $j$ of location $i$.

We can compute the SIR as follows (`expected()` was extracted from the `SpatialEpi` package)

```{r, message = F}
# Extracted from `SpatialEpi`
expected <- function (population, cases, n.strata, ...) {
  
  n <- length(population) / n.strata
  E <- rep(0, n)
  qNum <- rep(0, n.strata)
  qDenom <- rep(0, n.strata)
  q <- rep(0, n.strata)
  
  # Compute strata-specific rates
  for (i in 1:n.strata) {
    indices <- rep(i, n) + seq(0, (n - 1)) * n.strata
    qNum[i] <- qNum[i] + sum(cases[indices])
    qDenom[i] <- qDenom[i] + sum(population[indices])
  }
  q <- qNum / qDenom
  
  # Compute expected counts
  for (i in 1:n) {
    indices <- (1:n.strata) + (i - 1) * n.strata
    E[i] <- sum(population[indices] * q)
  }
  
  E
}
```

```{r}
################
# Process data #
################

# Observed number of cases
d <- d_ohio %>% group_by(NAME, year) %>% summarise(Y = sum(y)) %>% ungroup() %>% rename(county = NAME) %>% arrange(county, year)

# Expected cases 
d_ohio <- d_ohio %>% arrange(county, year, gender, race)
n.strata <- 4 # 2 genders x 2 races 
E <- expected(population = d_ohio$n, cases = d_ohio$y, n.strata = n.strata)

# Compute SIR
n_years    <- length(unique(d_ohio$year))
n_counties <- length(unique(d_ohio$NAME))

counties_E <- rep(unique(d_ohio$NAME), each = n_years)
years_E    <- rep(unique(d_ohio$year), times = n_counties)

d_E <- data.frame(county = counties_E, year = years_E, E = E)

d <- d %>% left_join(y = d_E, by = c("county", "year"))
d <- d %>% mutate(SIR = Y / E)

# Link map  information
d_wide <- d %>% pivot_wider(names_from = year, values_from = c(Y, E, SIR), names_glue = "{.value}.{year}") # `wide` format
m_ohio <- m_ohio %>% rename(county = NAME) %>% left_join(y = d_wide, by = c("county")) %>% dplyr::select(c(colnames(d_wide), "geometry"))

s_cols <- setdiff(colnames(m_ohio), c("county", "geometry")) # to `long` format
m_ohio <- m_ohio %>% 
          pivot_longer(cols = all_of(s_cols), names_to = c(".value", "year"), names_sep = "\\.") %>% 
          dplyr::select(county, year, Y, E, SIR, geometry) %>%
          mutate(year = as.numeric(year)) %>% 
          arrange(county, year)

head(m_ohio)
```

Now, we can plot the computed SIR for all locations for all years.

```{r}
ggplot(m_ohio) + 
  geom_sf(aes(fill = SIR)) +
  facet_wrap(~ year, ncol = 7) +
  scale_fill_gradient2(name = "SIR", midpoint = 1, low = "blue", mid = "white", high = "red") + 
  labs(x = "", y = "") +
  custom_theme + 
  theme(axis.text.x = element_blank(),
        axis.text.y = element_blank(),
        axis.ticks  = element_blank())
```

However, if there exist locations with small population or expected counts, SIR may be unreliable. To overcome this issue, we will model the relative risk as follows

\begin{align*}
Y_i &\sim \text{Poisson}(E_i \theta_i), \forall i,\\
\log(\theta_i) &= \alpha + \sum_{j = 1}^{n_f}f^{(j)}(u_{ij})
\end{align*}
where the relative risk $\theta_i$ quantifies whether the location $i$ has higher ($>1$) or lower ($<1$) risk than the average risk in the standard population. We will experiment with different structures (if any) for the random effects.

Aiming to implement the random effects, let us include an index for area and time in our data set.

```{r}
# Create indices for random effects
m_ohio <- m_ohio %>% mutate(id_area = as.numeric(as.factor(county)),
                            id_time = (1 + year - min(year)))
```

However, we will with data corresponding to $1988$ only (we will consider a spatio-temporal structure later). 

```{r}
data <- m_ohio %>% filter(year == 1988)
```

## No random effects

```{r}
formula_1 <- Y ~ 0 + 1
```

```{r, eval = F}
model_2_1 <- inla(formula = formula_1,
                  family  = "poisson", 
                  data = data, 
                  E = E, # Known component in the mean for the Poisson likelihoods
                  control.predictor = list(compute = TRUE),
                  control.compute   = list(cpo  = TRUE, 
                                           dic  = TRUE, 
                                           waic = TRUE)) # For model comparison
```

Notice that we are computing `cpo`, `dic`, and `waic`, so that we can compare models.

```{r, echo = F}
model_2_1 <- readRDS("models/model_2_1.RDS")
```

```{r}
summary(model_2_1)
```

For later use, let us create the function `table_model_comparison` to print a table for model comparison.

```{r}
table_model_comparison <- function (models, ...) {
  
  n_models <- length(models)
  df <- as.data.frame(matrix(data = 0, nrow = n_models, ncol = 3))
  rownames(df) <- names(models)
  colnames(df) <- c("CPO", "DIC", "WAIC")
  
  for (i in 1:n_models) {
    tmp_CPO  <- sum(log(models[[i]]$cpo$cpo)) * -1
    tmp_DIC  <- models[[i]]$dic$dic
    tmp_WAIC <- models[[i]]$waic$waic
    
    df[i, ] <- c(tmp_CPO, tmp_DIC, tmp_WAIC)
  }
  
  df
}
```

## IID model

```{r}
Y ~ 0 + 1 + f(id_area, model = "iid")
```

```{r, eval = F}
model_2_2 <- inla(formula = formula_2,
                  family  = "poisson", 
                  data = data, 
                  E = E, 
                  control.predictor = list(compute = TRUE),
                  control.compute   = list(cpo  = TRUE, 
                                           dic  = TRUE, 
                                           waic = TRUE)) 
```

```{r, echo = F}
model_2_2 <- readRDS("models/model_2_2.RDS")
```

```{r}
summary(model_2_2)
```

```{r}
# Model comparison
table_model_comparison(models = list(model_2_1 = model_2_1, model_2_2 = model_2_2))
```

## Intrinsic Conditional Auto-Regressive (ICAR) model

Alternatively, we can also account for the spatial dependence that may exist underlying our observed counts.

To do so, the first thing we have to do is determining the neighboring structure for the analyzed region. This can be done by using the `poly2nb()` function from the `spdep` package.

```{r}
nb <- poly2nb(data$geometry)
head(nb)

# Save matrix for later use within INLA
nb2INLA("data/example_2/map.adj", nb)
g <- inla.read.graph(filename = "data/example_2/map.adj")
```

Then we can fit a `iid + besag` model. 

**Remark:** in `INLA`, we can always use `inla.doc()` to view the documentation (e.g., parameterization) of models.

```{r, eval = F}
# Example: `besag`
inla.doc("besag")
```

In this case, we are fitting a model such that

\begin{align*}
\log(\theta_i) = \alpha + u_i + v_i,
\end{align*}
such that $u_i$ is a random effect specific to area $i$ to model spatial dependence between the relative risks, and $v_i$ is an unstructured exchangeable component that models uncorrelated noise.

```{r}
formula_3 <- Y ~ 0 + 1 + f(id_area, model = "iid") + f(id_area_cp, model = "besag", graph = g, scale.model = TRUE)
```

COMMENT ABOUT `scale.model = TRUE`.

```{r, eval = F}
model_2_3 <- inla(formula = formula_3,
                  family  = "poisson", 
                  data = data, 
                  E = E, 
                  control.predictor = list(compute = TRUE),
                  control.compute   = list(cpo  = TRUE, 
                                           dic  = TRUE, 
                                           waic = TRUE)) 
```

```{r, echo = F}
model_2_3 <- readRDS("models/model_2_3.RDS")
```

```{r}
summary(model_2_3)
```

```{r}
# Model comparison
table_model_comparison(models = list(model_2_1 = model_2_1, model_2_2 = model_2_2, model_2_3 = model_2_3))
```

## BYM2 model

As an alternative to `iid + besag`, we can fit a "Besag, York, and Mollié" (BYM2) model. In this case, we reparameterize the structured $u = (u_1, \cdots, u_n)$ and unstructured $v = (v_1, \cdots, v_n)$ random effects as follows
\begin{align*}
b = u + v = \frac{1}{\sqrt{\tau_b}}(\sqrt{1 - \phi}v^{\star} + \sqrt{\phi}u^{\star}),
\end{align*}
where $\tau_b > 0$ is the precision, $0 \leq \phi \leq 1$ is a mixing parameter, $v^{\star} \sim \text{Normal}(0, I_n)$ and $u^{\star}$ is a scaled ICAR model.

```{r}
formula_4 <- Y ~ 0 + 1 + f(id_area, model = "bym2", graph = g)
```


```{r, eval = F}
model_2_4 <- inla(formula = formula_4,
                  family  = "poisson", 
                  data = data, 
                  E = E, 
                  control.predictor = list(compute = TRUE),
                  control.compute   = list(cpo  = TRUE, 
                                           dic  = TRUE, 
                                           waic = TRUE)) 
```

```{r, echo = F}
model_2_4 <- readRDS("models/model_2_4.RDS")
```

```{r}
summary(model_2_4)
```

```{r}
# Model comparison
table_model_comparison(models = list(model_2_1 = model_2_1, model_2_2 = model_2_2, model_2_3 = model_2_3, model_2_4 = model_2_4))
```

### Penalized Complexity (PC) priors

As before, we can also set different priors before fitting our model. In particular, we can set Penalized Complexity (PC) priors for our `bym2` model.

As in [Gómez-Rubio (2021)](https://becarioprecario.bitbucket.io/inla-gitbook/ch-priors.html#sec:pcpriors),

> PC priors are designed following some principles for inference. First of all, the prior favors the base model unless evidence is provided against it, following the principle of parsimony. Distance from the base model is measured using the Kullback-Leibler distance, and penalization from the base model is done at a constant rate on the distance. Finally, the PC prior is defined using probability statements on the model parameters in the appropriate scale.

To define the prior for the marginal precision $\tau_b$ we use the probability statement $\mathbb{P}((1/\sqrt{\tau_b}) > U) = \alpha$. A prior for $\phi$ is defined using $\mathbb{P}(\phi < U) = \alpha$.

```{r}
pc_prior <- list(
  prec = list(
    prior = "pc.prec",
    param = c(1, 0.01)), # P(1 / sqrt(prec) > 1) = 0.01
  phi = list(
    prior = "pc",
    param = c(0.5, 0.75)) # P(phi < 0.5) = 0.75
)
```

From above, notice that $\mathbb{P}(\phi < 0.5) = 0.75$ implies that we believe that the unstructured effect accounts for more of the variability than the spatially structured effect.

```{r}
formula_5 <- Y ~ 0 + 1 + f(id_area, model = "bym2", graph = g, hyper = pc_prior)
```


```{r, eval = F}
model_2_5 <- inla(formula = formula_5,
                  family  = "poisson", 
                  data = data, 
                  E = E, 
                  control.predictor = list(compute = TRUE),
                  control.compute   = list(cpo  = TRUE, 
                                           dic  = TRUE, 
                                           waic = TRUE)) 
```

```{r, echo = F}
model_2_5 <- readRDS("models/model_2_5.RDS")
```

```{r}
summary(model_2_5)
```

```{r}
# Model comparison
table_model_comparison(models = list(model_2_1 = model_2_1, model_2_2 = model_2_2, model_2_3 = model_2_3, model_2_4 = model_2_4, model_2_5 = model_2_5))
```

## Fitted values

Although `model_2_4` and `model_2_5`, for the purpose of this tutorial, I plot the fitted values based on `model_2_3` (`iid + besag`), so that we can plot the estimated unstructured and structured effects separately.

We can retrieve the fitted values by analyzing `model_2_3$summary.fitted.values`.

```{r}
fitted_values <- model_2_3$summary.fitted.values[, c("mean", "0.025quant", "0.975quant")] %>% 
                 as_tibble() %>% 
                 mutate(id_area = 1:nrow(data)) %>% 
                 rename(Mean = mean, `0.025` = `0.025quant`, `0.975` = `0.975quant`) %>% 
                 left_join(y = data[, c("county", "geometry", "id_area")], by = "id_area") %>% 
                 dplyr::select(county, Mean, `0.025`, `0.975`, geometry) %>% 
                 pivot_longer(cols = c(Mean, `0.025`, `0.975`)) %>% 
                 mutate(name = factor(name, levels = c("0.025", "Mean", "0.975"))) %>% 
                 st_as_sf()

ggplot(fitted_values) + 
  geom_sf(aes(fill = value)) +
  facet_wrap(~ name, dir = "h", ncol = 3) +
  scale_fill_gradient2(name = "Relative risk", midpoint = 1, low = "blue", mid = "white", high = "red") + 
  labs(x = "", y = "", title = "Estimated \"Relative Risk\" in 1988") +
  custom_theme + 
  theme(axis.text.x = element_blank(),
        axis.text.y = element_blank(),
        axis.ticks  = element_blank())
```

And we can retrieve the summary of the posterior distribution of the random effects by analyzing `model_2_3$summary.random`.

```{r}
random_effects <- data[, c("county", "geometry")] %>% 
                  mutate("IID" = model_2_3$summary.random$id_area$mean,
                         "BESAG" = model_2_3$summary.random$id_area_cp$mean) %>% 
                  pivot_longer(cols = c(IID, BESAG)) %>% 
                  mutate(name = factor(name, levels = c("IID", "BESAG"))) %>% 
                  st_as_sf()

ggplot(random_effects) + 
  geom_sf(aes(fill = value)) +
  facet_wrap(~ name, dir = "h", ncol = 2) +
  scale_fill_gradient2(name = "RE", midpoint = 0, low = "blue", mid = "white", high = "red") + 
  labs(x = "", y = "", title = "Posterior mean of the random effects") +
  custom_theme + 
  theme(axis.text.x = element_blank(),
        axis.text.y = element_blank(),
        axis.ticks  = element_blank())
```

# Spatio-temporal models

We will continue analyzing the data set we introduced in the previous section (i.e., number of cases of lung cancer in Ohio, USA, from 1968 to 1988), but now we will also account for the possible temporal dependence that may exist over the years.

```{r}
data <- m_ohio
head(data)
```

Similar to before, we will model the relative risk as follows
\begin{align*}
Y_{it} &\sim \text{Poisson}(E_{it} \theta_{it}), \forall i, t, \\
\log(\theta_{it}) &= \alpha + \sum_{j = 1}^{n_f}f^{(j)}(u_{it,j}).
\end{align*}
However, notice that now all quantities depend on the year (or time, $t$). The random effects may depend on both space and time with possible interaction.

## Replicates

An alternative to model space-time observations is treating them as replicates. In `INLA`, using `replicate` implies that replicated effects share the hyperparameters (only). This means that the values of the random effects in the different replicates can be different ([Gómez-Rubio, 2021](https://becarioprecario.bitbucket.io/inla-gitbook/ch-INLAfeatures.html#subsec:replicate)).

```{r}
formula_1 <- Y ~ 0 + 1 + f(id_area, model = "besag", graph = g, replicate = id_time)
```

```{r, eval = F}
model_3_1 <- inla(formula = formula_1,
                  family  = "poisson", 
                  data = data, 
                  E = E,
                  control.predictor = list(compute = TRUE))
```

```{r, echo = F}
model_3_1 <- readRDS("models/model_3_1.RDS")
```

```{r}
summary(model_3_1)
```

Aiming to plot the fitted values for the different spatio-temporal approaches, we will use the function `plot_rr_years()`.

```{r}
plot_rr_years <- function (d, col_name = "fitted.values", temporal_name = "year", tt = "", ...) {
  ggplot(d) + 
    geom_sf(aes(fill = .data[[col_name]])) +
    facet_wrap(~ .data[[temporal_name]], dir = "h", ncol = 7) +
    scale_fill_gradient2(name = "Relative risk", midpoint = 1, low = "blue", mid = "white", high = "red") + 
    labs(x = "", y = "", title = tt) +
    custom_theme + 
    theme(axis.text.x = element_blank(),
          axis.text.y = element_blank(),
          axis.ticks  = element_blank())
}
```

```{r}
data_3_1 <- bind_cols(data[, c("county", "year", "geometry")], fitted.values = model_3_1$summary.fitted.values$mean)

plot_rr_years(d = data_3_1, tt = "Replicates")
```

## Additive model

Alternatively, we can fit a model with an additive structure for random effects in space and time. For instance,

\begin{align*}
\log(\theta_{it}) = \alpha + u_i + \varrho_t,
\end{align*}
where $u_i \sim \text{ICAR}$ and, e.g., $\varrho_t \sim \text{AR}(1)$.

```{r}
formula_2 <- Y ~ 0 + 1 + f(id_time, model = "rw1") + f(id_area, model = "besag", graph = g)
```

```{r, eval = F}
model_3_2 <- inla(formula = formula_2,
                  family  = "poisson", 
                  data = data, 
                  E = E,
                  control.predictor = list(compute = TRUE))
```

```{r, echo = F}
model_3_2 <- readRDS("models/model_3_2.RDS")
```

```{r}
summary(model_3_2)
```

```{r}
data_3_2 <- bind_cols(data[, c("county", "year", "geometry")], fitted.values = model_3_2$summary.fitted.values$mean)

plot_rr_years(d = data_3_2, tt = "Additive model")
```

## Kronecker product model

A more elaborated solution is to consider a spatio-temporal random effect. In particular, we will fit a grouped (or Kronecker) separable model. A more rigorous motivation for this approach is available [here](https://github.com/hrue/r-inla/blob/devel/internal-doc/group/group-models.pdf).

In a nutshell, in the grouped random effected,

1. There is a **within** group correlation structure, and
2. There is a **between** group correlation structure.

In particular, if $y_{g, i}$ is the $i$-the element in group $g$, then $\text{Cov}(y_{g_1, i_1}, y_{g_2, i_2}) = (\text{cov. between groups } g_1 \text{ and } g_2) \times (\text{cov. between elements } g_1 \text{ and } g_2)$ ([Simpson, 2016](http://faculty.washington.edu/jonno/SISMIDmaterial/8-Groupedmodels.pdf)).

In `INLA`, the within-group effect is the one defined in the main `f()` function, while the between effect is the one defined using the `control.group` argument.

```{r}
formula_3 <- Y ~ 0 + 1 + f(id_area, model = "besag", graph = g, 
                           group = id_time, control.group = list(model = "rw1"))
```

```{r, eval = F}
model_3_3 <- inla(formula = formula_3,
                  family  = "poisson", 
                  data = data, 
                  E = E,
                  control.predictor = list(compute = TRUE))
```

```{r, echo = F}
model_3_3 <- readRDS("models/model_3_3.RDS")
```

```{r}
summary(model_3_3)
```

```{r}
data_3_3 <- bind_cols(data[, c("county", "year", "geometry")], fitted.values = model_3_3$summary.fitted.values$mean)

plot_rr_years(d = data_3_3, tt = "Kronecker product model (grouped by \"time\")")
```

## Bernardinelli model

Lastly, we can also implement the Bernardinelli model ([Bernardinelli et al., 1995](https://onlinelibrary.wiley.com/doi/10.1002/sim.4780142112)) using `INLA`. In this case, the linear predictor will be defined as follows

\begin{align*}
\log(\theta_{it}) = \alpha + u_i + v_i + (\beta + \delta_i) \times t_j,
\end{align*}
where $u_i + v_i$ is a `iid + besag` model, $\beta$ is the global linear trend, and $\delta_i$ denotes a space-time interaction; in particular, this model allows each of the areas to have its own time trend with spatial intercept given by $(\alpha + u_i + v_i)$ and slope given by $(\beta + \delta_i)$ ([Moraga, 2019](https://www.paulamoraga.com/book-geospatial/sec-arealdatatheory.html#spatio-temporal-small-area-disease-risk-estimation)).

```{r}
formula_4 <- Y ~ 0 + 1 + f(id_area, model = "bym", graph = g) + f(id_area_cp, id_time, model = "iid") + id_time
```

```{r, eval = F}
model_3_4 <- inla(formula = formula_4,
                  family  = "poisson", 
                  data = data, 
                  E = E,
                  control.predictor = list(compute = TRUE))
```

```{r, echo = F}
model_3_4 <- readRDS("models/model_3_4.RDS")
```

```{r}
summary(model_3_4)
```

```{r}
data_3_4 <- bind_cols(data[, c("county", "year", "geometry")], fitted.values = model_3_4$summary.fitted.values$mean)

plot_rr_years(d = data_3_4, tt = "Bernardinelli model")
```

# Geostatistical data 

Throughout this section, we will assume that
\begin{align*}
    y_i = \mu + \zeta(x_i), ~\forall i,
\end{align*}
where $y = (y_1, \cdots, y_n)$ are the observations at $x = (x_1, \cdots, x_n)$, $\mu$ is the common mean, and $\zeta(x)$ is a zero-mean stationary and isotropic Gaussian random field. In particular, $\zeta(x)$ has a [Matérn structure](https://becarioprecario.bitbucket.io/spde-gitbook/ch-intro.html#sec:matern).

Other more sophisticated models are described in [Advanced Spatial Modeling with Stochastic Partial Differential Equations Using R and INLA](https://becarioprecario.bitbucket.io/spde-gitbook/index.html) ([Krainski et al., 2019](https://becarioprecario.bitbucket.io/spde-gitbook/index.html)).

For fitting such a model, we can rely on a stochastic partial differential equation (SPDE) approach. As showed in [Whittle](.) ([1963](.)), a Gaussian random field with Matérn covariance structure can be expressed as a solution of the following SPDE
\begin{align*}
    (\kappa^2 - \Delta)^{\alpha/2}(\tau S(x)) = \mathcal{W}(x),
\end{align*} 
where $\Delta$ is the Laplacian, $\mathcal{W}(s)$ is a Gaussian white-noise random process, $\alpha$ controls the smoothness of the random field, $\tau$ controls the variance, and $\kappa$ is a scale parameter. 

Based on this result, [Lindgren et al.](https://academic.oup.com/jrsssb/article/73/4/423/7034732) ([2011](https://academic.oup.com/jrsssb/article/73/4/423/7034732)) proposed a new procedure for representing a Gaussian random field with Matérn covariance as a Gaussian Markov Random Field (GMRF). They did this by expressing a solution of the SPDE using the Finite Element Method (FEM). 

The smoothness parameter $\nu$ of the Matérn covariance structure is related to $\alpha$ in the SPDE formulation through $\alpha = \nu + (d / 2)$, where $d$ (typically $d = 2$) is dimension. In `INLA`, $0 \leq \alpha < 2$.

## PM2.5

We will analyze the particulate matter 2.5 (PM2.5), measured in $\mu \text{g} / \text{m}^3)$ levels in the United States
(USA) in 2022. The data is the same as the one analyzed in [Amaral et al.](https://link.springer.com/article/10.1007/s13253-023-00571-0) ([2023](https://link.springer.com/article/10.1007/s13253-023-00571-0)). The data can be downloaded from [here](.).

```{r}
data_USA <- readRDS(file = "data/example_4/data_USA.rds")
USA <- readRDS(file = "data/example_4/USA_filtered.rds")

head(data_USA)

ggplot() + 
  geom_sf(data = USA, fill = "white") +
  geom_sf(data = data_USA, aes(fill = mean), color = "black", size = 3, shape = 21) +
  scale_fill_gradientn(name = "PM2.5 level", colors = rainbow(9, start = 0.1, end = 0.9)) + 
  labs(x = "", y = "", title = "") +
  custom_theme + 
  theme(axis.text.x = element_blank(),
        axis.text.y = element_blank(),
        axis.ticks  = element_blank())
```

Now, let us pre-process the data.

```{r}
# Boundary coordinates
USA_coor <- sf::st_coordinates(USA)
USA_coor <- matrix(c(USA_coor[, 1], USA_coor[, 2]), ncol = 2)
colnames(USA_coor) <- c("lon", "lat")
head(USA_coor)

# Points coordinates
data_coor <- sf::st_coordinates(data_USA)
data_USA  <- bind_cols(data_USA, as_tibble(data_coor))
data_USA  <- data_USA %>% rename(lon = X, lat = Y) %>% dplyr::select(mean, sd, lon, lat, geometry)
head(data_USA)
```

And create the mesh with the `inla.mesh.2d()` function. 

```{r}
# `max.edge`: the largest allowed triangle edge length. One or two values.
# `offset`: the automatic extension distance. One or two values, for an inner and an optional outer extension.
mesh <- inla.mesh.2d(loc.domain = USA_coor, max.edge = c(300, 3000), offset = c(300, 1500))

mesh$n # Number of nodes

# Plot `mesh`
{
  plot(mesh)
  plot(USA$geometry, lwd = 2, border = "red", add = TRUE)
  points(data_USA$lon, data_USA$lat, pch = 1, col = "green")
}
```

We can build the SPDE model using one of the following functions: `inla.spde2.matern()` or `inla.spde2.pcmatern`. Notice that the parameterization of these two models is different, such that the latter accepts PC priors.

```{r}
# Building the SPDE model
# alpha = nu + d / 2 = 1 + 1 = 2, for nu = 1

# Flexible parameterization (with a default one)
# spde <- inla.spde2.matern(mesh = mesh, alpha = 2)
# Parameterization for PC priors
spde <- inla.spde2.pcmatern(mesh = mesh, 
                            alpha = 2,
                            prior.range = c(1e3, 0.90), # P(range < 1e3) = 0.90
                            prior.sigma = c(1.0, 0.01)) # P(sigma > 1.0) = 0.01
```

Now, we can create an `indxs` object for the SPDE model, as well as the projection matrix. The projection matrix depends on the `mesh` and `data_coor` and is used to project the Gaussian random field (GRF) from the observations to the triangulation vertices.

```{r}
# Indices
indxs <- inla.spde.make.index("s", spde$n.spde)
# Projection matrix
A <- inla.spde.make.A(mesh = mesh, loc = data_coor)
dim(A)
```

To make prediction, we have to define the set of coordinates where we want to estimate the process. Given location boundaries and the desired resolution, the function `create_prediction_grid()` does the job.

```{r}
create_prediction_grid <- function (loc, resolution = 25, ...) {
  resolution <- 25 
  pts_bdy <- loc$geometry[[1]][[1]]
  pts_bdy_x <- range(pts_bdy[, 1])
  pts_bdy_y <- range(pts_bdy[, 2])
  coord_pred <- expand.grid(x = seq(pts_bdy_x[1], pts_bdy_x[2], by = resolution), y = seq(pts_bdy_y[1], pts_bdy_y[2], by = resolution))
  coordinates(coord_pred) <- ~ x + y
  
  xx <- as(st_as_sf(coord_pred), "sf");   st_crs(xx) <- st_crs(loc$geometry)
  yy <- as(st_as_sf(loc$geometry), "sf"); st_crs(yy) <- st_crs(loc$geometry)
  
  # Compute intersection between grid and `loc` borders
  pppts <- st_intersection(x = xx, y = yy)
  
  coord_pred <- matrix(data = NA, nrow = length(pppts$geometry), ncol = 2)
  colnames(coord_pred) <- c("x", "y")
  for (p in 1:length(pppts$geometry)) { coord_pred[p, ] <- sf::st_coordinates(pppts$geometry[[p]]) }
  coord_pred <- data.frame(x = coord_pred[, 1], y = coord_pred[, 2])
  
  as.matrix(coord_pred)
}
```

```{r, eval = F}
coord_pred <- create_prediction_grid(USA)
```

```{r, echo = F}
coord_pred <- readRDS(file = "data/example_4/coord_pred.RDS")
```

```{r}
{
  coord_pred_cp <- as.data.frame(coord_pred)
  coordinates(coord_pred_cp) <- ~ x + y
  plot(coord_pred_cp)
}
```

Similar to before, we must create a projection matrix for the locations where we want to make prediction.

```{r}
# Projection matrix for prediction
Ap <- inla.spde.make.A(mesh = mesh, loc = coord_pred)
```

Lastly, we can organize the data in stacks. 

```{r}
# Create stacks
# Stack for estimation 
stk_e <- inla.stack(tag = "est",
                    data = list(y = data_USA$mean),
                    A = list(1, A),
                    effects = list(data.frame(b0 = rep(1, nrow(data_USA))), s = indxs))

# Stack for prediction
stk_p <- inla.stack(tag = "pred",
                    data = list(y = NA),
                    A = list(1, Ap),
                    effects = list(data.frame(b0 = rep(1, nrow(coord_pred))), s = indxs))

# Full stack
stk_full <- inla.stack(stk_e, stk_p)
```

After all these steps, we can finally fit the model.

```{r}
formula_1 <- y ~ 0 + b0 + f(s, model = spde)
```

```{r, eval = FALSE}
model_4_1 <- inla(formula = formula_1,
                  family  = "gaussian", 
                  data = inla.stack.data(stk_full), 
                  control.predictor = list(compute = TRUE,
                                           A = inla.stack.A(stk_full))) # Matrix of predictors
```

```{r, echo = F}
model_4_1 <- readRDS(file = "models/model_4_1.RDS")
```

```{r}
summary(model_4_1)
```

As we did in the "Multiple linear regression" example, we can work with the posterior marginals.

```{r}
ss <- inla.tmarginal(fun = function(x) { 1 / sqrt(x) }, marginal = model_4_1$marginals.hyperpar$`Precision for the Gaussian observations`)

ggplot(data.frame(inla.smarginal(ss))) +
  geom_line(aes(x, y)) +
  labs(x = "", y = "", title = "Posterior of the standard deviation for the observations") + 
  custom_theme

ggplot(data.frame(inla.smarginal(model_4_1$marginals.hyperpar$`Range for s`))) +
  geom_line(aes(x, y)) +
  labs(x = "", y = "", title = "Posterior of the range in the Matérn model") + 
  custom_theme

ggplot(data.frame(inla.smarginal(model_4_1$marginals.hyperpar$`Stdev for s`))) +
  geom_line(aes(x, y)) +
  labs(x = "", y = "", title = "Posterior of the standard deviation in the Matérn model") + 
  custom_theme
```

**Remark:** According to [this parameterization](https://becarioprecario.bitbucket.io/spde-gitbook/ch-intro.html#sec:matern), the scale parameter $\kappa > 0$ can be expressed by $\frac{\sqrt{8\nu}}{\text{range}}$, where $\nu$ is the smoothness parameter.

### Prediction

Now, let us retrieve the predicted values. 

```{r}
# Fitted values and prediction

# Indices for the predicted observations
idxs_pred <- inla.stack.index(stk_full, tag = "pred")$data

pred_mm <- as.data.frame(cbind(coord_pred, model_4_1$summary.fitted.values[idxs_pred, "mean"]))
pred_ll <- as.data.frame(cbind(coord_pred, model_4_1$summary.fitted.values[idxs_pred, "0.025quant"]))
pred_uu <- as.data.frame(cbind(coord_pred, model_4_1$summary.fitted.values[idxs_pred, "0.975quant"]))

# E.g.,
pred_mm %>% as_tibble() %>% rename(posterior_mean = V3) %>% head() 
```

To plot the posterior mean and percentiles, we will use the function `plot_pred_USA()`. 

```{r}
plot_pred_USA <- function (fitted_values, USA, r, tt = "", ...) {
  
  coordinates(fitted_values) <- ~ x + y
  gridded(fitted_values) <- TRUE
  fitted_values <- raster(fitted_values)
  crs(fitted_values) <- "+init=epsg:6345 +units=km +no_defs"
  
  fitted_values    <- as(fitted_values, "SpatialPixelsDataFrame")
  fitted_values_df <- as.data.frame(fitted_values)
  colnames(fitted_values_df) <- c("pred", "x", "y")
  
  breaks <- seq(floor(r[1]), ceiling(r[2]), length.out = 5)
  pp <- ggplot() +
    geom_tile(data = fitted_values_df, mapping = aes(x = x, y = y, fill = pred)) + 
    geom_sf(data = USA, color = "black", fill = NA, lwd = 0.5) +
    scale_fill_gradientn(name = "PM2.5", colors = rainbow(9, start = 0.1, end = 0.9), breaks = breaks, limits = c(breaks[1], tail(breaks, 1))) + 
    labs(x = "", y = "", title = tt) + 
    custom_theme + 
    theme(axis.text.x = element_blank(),
          axis.text.y = element_blank(),
          axis.ticks  = element_blank())
  
  pp
} 
```

The `patchwork` package allows us to combine plots.

```{r, warning = FALSE}
r_mm <- pred_mm$V3 %>% range()
r_ll <- pred_ll$V3 %>% range()
r_uu <- pred_uu$V3 %>% range()
r <- c(min(r_mm[1], r_ll[1], r_uu[1]), max(r_mm[2], r_ll[2], r_uu[2]))

pp_mm <- plot_pred_USA(fitted_values = pred_mm, USA = USA, r = r, tt = "Mean")
pp_ll <- plot_pred_USA(fitted_values = pred_ll, USA = USA, r = r, tt = "2.5th")
pp_uu <- plot_pred_USA(fitted_values = pred_uu, USA = USA, r = r, tt = "97.5th")

(pp_ll + pp_mm + pp_uu) + plot_layout(guides = "collect") & theme(legend.position = "right")
```

# Point processes

In this section, we will see how to model a log-Gaussian Cox process (LGCP) using `INLA`. However, one can refer to [this tutorial](https://avramaral.github.io/PP_inference/) for a more detailed explanation on how to make inference in point processes (also using `INLA`).

Recall that

> A Cox process can be seen as a doubly stochastic process. $\xi$ is a Cox process driven by $\Lambda(x)$ if
>
> 1. $\{\Lambda(x); x \in \mathcal{D}\}$ is a non-negative valued stochastic process.
> 
> 2. Conditional on $\{\Lambda(x) = \lambda(x); \mathbf{x} \in \mathcal{D}\}$, $\xi$ is a Poisson process with intensity function $\lambda(x)$.

A particular case of a Cox process, named **log-Gaussian Cox process**, can be constructed by setting $\log(\Lambda(x)) = \mu^{\star}(x) + \zeta(x)$, such that $\mu(x) = \exp(\mu^{\star}(x))$ is possibly interpreted as the mean structure of $\Lambda(x)$, and $\zeta(x)$ is a stationary Gaussian process, such that $\mathbb{E}(\zeta(x)) = -\sigma^2/2$, $\forall x$, and $\text{Cov}(\zeta(x_1), \zeta(x_2)) = \phi(h) = \sigma^2 \rho(h)$, where $h$ is the distance between $x_1$ and $x_2$, and $\sigma^2$ is the variance of $\zeta(x)$.

---

As we can see [here](https://avramaral.github.io/PP_inference/#fitting-a-lgcp-with-r-inla), a way to fit a LGCP with `INLA` is placing a regular grid, given by cells $\{c_{ij}\}_{ij}$, on top of the spatial domain, such that $\int_{c_{i,j}}\Lambda(x)dx \approx |c_{i,j}|\Lambda(x)$, where $|\cdot|$ denotes the area.

Thus, conditional on $\zeta(x)$, the number of locations in the grid cell $c_{ij}$, $\forall i, j$, are independent and Poisson distributed, such that
\begin{align*}
\mathcal{N}(c_{ij})|\zeta(x) &\sim \text{Poisson}(|c_{ij}| \Lambda(x_{ij})), \forall i, j, \\
\log(\Lambda(x_{ij})) &= \alpha + \cdots + \zeta(x_{ij})
\end{align*}
where $\zeta(x)$ is a Gaussian field (e.g., `matern2d`).

However, [Simpson et al.](https://arxiv.org/abs/1111.0641)([2016](https://arxiv.org/abs/1111.0641)) proposed a method to make inference on LGCP while still considering the exact observation locations---instead of aggregating them over a regular grid and fitting a counting model for each cell. 

The main idea consists of a direct approximation of the likelihood by fitting a Poisson model on an augmented data set made of the observation locations and integration points from the corresponding SPDE mesh used to represent the original spatial process.

## PM2.5 stations

Although the problem of estimating PM2.5 depends, obviously, on the observed pollution levels, we will ignore it for now and focus on the sampling process, i.e., the locations where the monitoring stations were placed and treat them as a realization of a random process. In particular, a LGCP.

```{r}
data_USA <- readRDS(file = "data/example_4/data_USA.rds")
USA <- readRDS(file = "data/example_4/USA_filtered.rds")

ggplot() + 
  geom_sf(data = USA, fill = "white") +
  geom_sf(data = data_USA, color = "black", size = 3, shape = 3) +
  labs(x = "", y = "", title = "") +
  custom_theme + 
  theme(axis.text.x = element_blank(),
        axis.text.y = element_blank(),
        axis.ticks  = element_blank())
```

